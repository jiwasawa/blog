<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Junichiro Iwasawa">
<meta name="dcterms.date" content="2025-04-17">

<title>Introduction to Diffusion Models: From Core Concepts to Cutting-Edge Applications</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-12858c9ce0885248945210bbd3c85d1a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Home</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Junichiro Iwasawa</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/jiwasawa"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/junichiro-iwasawa-875b37130/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jiwasawa"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Introduction to Diffusion Models: From Core Concepts to Cutting-Edge Applications</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Machine Learning</div>
                <div class="quarto-category">Diffusion models</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Junichiro Iwasawa </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 17, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>In recent years, diffusion models have taken the generative modeling world by storm, particularly in image synthesis, often producing stunning results. This post aims to provide a comprehensive introduction, starting from the fundamental concepts and moving towards the advanced techniques that power today’s state-of-the-art models.</p>
<section id="what-are-diffusion-models" class="level2">
<h2 class="anchored" data-anchor-id="what-are-diffusion-models">What are Diffusion Models?</h2>
<p>Diffusion models are a class of generative models. While other approaches like Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Flow-based models have achieved significant success, they each come with their own challenges—GANs can suffer from training instability, VAEs rely on surrogate loss functions, and Flow models require specialized architectures for reversible transformations.</p>
<p>Inspired by non-equilibrium thermodynamics, diffusion models offer a different paradigm. They work through two main processes:</p>
<ol type="1">
<li><strong>Forward Process (Diffusion Process):</strong> Gradually add small amounts of random noise (typically Gaussian) to the data over many steps, eventually transforming the data distribution into a simple, known distribution (like a standard Gaussian).</li>
<li><strong>Reverse Process (Denoising Process):</strong> Learn to reverse the diffusion process. Starting from pure noise, incrementally remove the noise step-by-step to generate a sample that belongs to the original data distribution.</li>
</ol>
<p>The core of the generative capability lies in the neural network trained to perform this “denoising” at each step. Diffusion models feature a fixed training procedure and, unlike VAEs or Flow models, typically operate with latent variables that have the same dimensionality as the original data.</p>
<section id="the-forward-process-turning-data-into-noise" class="level3">
<h3 class="anchored" data-anchor-id="the-forward-process-turning-data-into-noise">The Forward Process: Turning Data into Noise</h3>
<p>Starting with an initial data point <span class="math inline">\(\mathbf{x}_0\)</span> drawn from the true data distribution <span class="math inline">\(q(\mathbf{x})\)</span>, the forward process defines a Markov chain that adds Gaussian noise over <span class="math inline">\(T\)</span> discrete time steps. The transition at each step <span class="math inline">\(t\)</span> is defined as:</p>
<p><span class="math display">\[q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I})\]</span></p>
<p>Here, <span class="math inline">\(\\{\beta_t \in (0, 1)\\}_{t=1}^T\)</span> is a <strong>variance schedule</strong>, a set of hyperparameters controlling the amount of noise added at each step. Typically, <span class="math inline">\(\beta_t\)</span> increases with <span class="math inline">\(t\)</span>, meaning more noise is added in later steps (common schedules include linear and cosine [Nichol &amp; Dhariwal, 2021]). <span class="math inline">\(\mathbf{I}\)</span> is the identity matrix.</p>
<p>The joint distribution over all noisy samples given the initial data is:</p>
<p><span class="math display">\[q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) = \prod^T_{t=1} q(\mathbf{x}_t \vert \mathbf{x}_{t-1})\]</span></p>
<p>A key property of this process is that we can sample the noisy version <span class="math inline">\(\mathbf{x}_t\)</span> at any arbitrary timestep <span class="math inline">\(t\)</span> directly from the original data <span class="math inline">\(\mathbf{x}_0\)</span> in a closed form. Defining <span class="math inline">\(\alpha_t = 1 - \beta_t\)</span> and <span class="math inline">\(\bar{\alpha}_t = \prod_{i=1}^t \alpha_i\)</span>, the distribution of <span class="math inline">\(\mathbf{x}_t\)</span> given <span class="math inline">\(\mathbf{x}_0\)</span> is:</p>
<p><span class="math display">\[q(\mathbf{x}_t \vert \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})\]</span></p>
<p>This can also be written using the reparameterization trick: <span class="math inline">\(\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}\)</span>, where <span class="math inline">\(\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</span> is standard Gaussian noise. Essentially, <span class="math inline">\(\mathbf{x}_t\)</span> is a scaled version of the original data plus scaled noise. As <span class="math inline">\(T \to \infty\)</span>, <span class="math inline">\(\bar{\alpha}_T \approx 0\)</span>, and <span class="math inline">\(\mathbf{x}_T\)</span> becomes almost pure Gaussian noise <span class="math inline">\(\mathcal{N}(\mathbf{0}, \mathbf{I})\)</span>, independent of the starting <span class="math inline">\(\mathbf{x}_0\)</span>.</p>
</section>
<section id="the-reverse-process-from-noise-back-to-data" class="level3">
<h3 class="anchored" data-anchor-id="the-reverse-process-from-noise-back-to-data">The Reverse Process: From Noise back to Data</h3>
<p>The generative process reverses the forward diffusion. We start by sampling pure noise <span class="math inline">\(\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</span> and then iteratively sample <span class="math inline">\(\mathbf{x}_{t-1}\)</span> given <span class="math inline">\(\mathbf{x}_t\)</span> for <span class="math inline">\(t = T, T-1, \dots, 1\)</span> to eventually obtain a sample <span class="math inline">\(\mathbf{x}_0\)</span>.</p>
<p>To do this, we need the reverse transition probability <span class="math inline">\(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t)\)</span>. However, this distribution is intractable because it depends on the entire dataset. Therefore, we approximate it using a parameterized neural network, typically denoted as <span class="math inline">\(p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)\)</span>.</p>
<p>The entire reverse process is defined as:</p>
<p><span class="math display">\[p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T) \prod^T_{t=1} p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)\]</span></p>
<p>where the starting noise distribution is <span class="math inline">\(p(\mathbf{x}_T) = \mathcal{N}(\mathbf{x}_T; \mathbf{0}, \mathbf{I})\)</span>. Each reverse transition step <span class="math inline">\(p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)\)</span> is usually modeled as a Gaussian distribution:</p>
<p><span class="math display">\[p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t))\]</span></p>
<p>The goal of the model is to learn the mean <span class="math inline">\(\boldsymbol{\mu}_\theta(\mathbf{x}_t, t)\)</span> and the covariance <span class="math inline">\(\boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t)\)</span> of this reverse transition. In practice, the covariance <span class="math inline">\(\boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t)\)</span> is often not learned directly but is set to a fixed diagonal matrix <span class="math inline">\(\sigma_t^2 \mathbf{I}\)</span>. Common choices for <span class="math inline">\(\sigma_t^2\)</span> include <span class="math inline">\(\beta_t\)</span> (from the forward process) or <span class="math inline">\(\tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \beta_t\)</span> (derived theoretically). While learning the variance was explored by [Nichol &amp; Dhariwal, 2021] (e.g., as an interpolation between <span class="math inline">\(\beta_t\)</span> and <span class="math inline">\(\tilde{\beta}_t\)</span>), it can sometimes lead to instability.</p>
</section>
<section id="the-learning-objective-predicting-the-noise" class="level3">
<h3 class="anchored" data-anchor-id="the-learning-objective-predicting-the-noise">The Learning Objective: Predicting the Noise</h3>
<p>How do we train the network to learn <span class="math inline">\(\boldsymbol{\mu}_\theta(\mathbf{x}_t, t)\)</span>? While the full derivation involves maximizing the Variational Lower Bound (VLB) on the data log-likelihood, the DDPM paper [Ho et al., 2020] introduced a simpler, more intuitive objective that works remarkably well in practice.</p>
<p>The core idea is to reparameterize the model. Instead of directly predicting the mean <span class="math inline">\(\boldsymbol{\mu}_\theta\)</span> of the reverse step, the model is trained to <strong>predict the noise component <span class="math inline">\(\boldsymbol{\epsilon}\)</span> that was added to the original data <span class="math inline">\(\mathbf{x}_0\)</span> to produce <span class="math inline">\(\mathbf{x}_t\)</span> during the forward process.</strong> Let’s denote this noise-predicting model as <span class="math inline">\(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\)</span>.</p>
<p>Using the relationship <span class="math inline">\(\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}\)</span>, we can express the (true) mean of the reverse step <span class="math inline">\(\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0)\)</span> (which we could compute if we knew <span class="math inline">\(\mathbf{x}_0\)</span>) in terms of this noise <span class="math inline">\(\boldsymbol{\epsilon}\)</span>. We train our model <span class="math inline">\(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\)</span> to predict this true noise <span class="math inline">\(\boldsymbol{\epsilon}\)</span>, which in turn allows us to estimate the desired mean <span class="math inline">\(\boldsymbol{\mu}_\theta(\mathbf{x}_t, t)\)</span>.</p>
<p>Specifically, the learned mean <span class="math inline">\(\boldsymbol{\mu}_\theta(\mathbf{x}_t, t)\)</span> is parameterized using the predicted noise <span class="math inline">\(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\)</span> as follows:</p>
<p><span class="math display">\[\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \right)\]</span></p>
<p>This equation shows that learning the noise prediction model <span class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span> is sufficient to determine the mean of the reverse step.</p>
<p>The simplified training objective proposed in DDPM is then simply to minimize the Mean Squared Error (MSE) between the predicted noise <span class="math inline">\(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\)</span> and the actual noise <span class="math inline">\(\boldsymbol{\epsilon}\)</span> that was added:</p>
<p><span class="math display">\[L_\text{simple} = \mathbb{E}_{t \sim \mathcal{U}(1, T), \mathbf{x}_0 \sim q(\mathbf{x}_0), \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})} \left[\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}, t)\|^2 \right]\]</span></p>
<p><strong>Training Loop:</strong> 1. Sample a real data point <span class="math inline">\(\mathbf{x}_0 \sim q(\mathbf{x}_0)\)</span>. 2. Sample a random timestep <span class="math inline">\(t\)</span> uniformly from <span class="math inline">\(\\{1, \dots, T\\}\)</span>. 3. Sample a standard Gaussian noise <span class="math inline">\(\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</span>. 4. Compute the noisy version <span class="math inline">\(\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}\)</span>. 5. Feed <span class="math inline">\(\mathbf{x}_t\)</span> and <span class="math inline">\(t\)</span> into the model <span class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span> to get the noise prediction <span class="math inline">\(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\)</span>. 6. Calculate the MSE loss between <span class="math inline">\(\boldsymbol{\epsilon}\)</span> and <span class="math inline">\(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\)</span>. 7. Update the model parameters <span class="math inline">\(\theta\)</span> using gradient descent on this loss.</p>
<p><strong>Connection to Score Matching:</strong> Interestingly, this noise prediction task is closely related to score matching. The score function of a distribution <span class="math inline">\(q(\mathbf{x})\)</span> is defined as the gradient of its log-probability density, <span class="math inline">\(\nabla_{\mathbf{x}} \log q(\mathbf{x})\)</span>. The predicted noise <span class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span> is approximately proportional to the score of the noisy data distribution at time <span class="math inline">\(t\)</span>: <span class="math inline">\(\mathbf{s}_\theta(\mathbf{x}_t, t) \approx \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t) \approx - \frac{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1 - \bar{\alpha}_t}}\)</span>. This links diffusion models to score-based generative models like NCSN [Song &amp; Ermon, 2019].</p>
</section>
</section>
<section id="evolution-and-applications-of-diffusion-models" class="level2">
<h2 class="anchored" data-anchor-id="evolution-and-applications-of-diffusion-models">Evolution and Applications of Diffusion Models</h2>
<p>Following the success of DDPM, research has focused on improving performance, expanding capabilities, and addressing limitations.</p>
<section id="conditional-generation" class="level3">
<h3 class="anchored" data-anchor-id="conditional-generation">Conditional Generation</h3>
<p>Generating samples based on specific information like class labels, text descriptions, or other images.</p>
<ul>
<li><p><strong>Classifier Guidance:</strong> Proposed by [Dhariwal &amp; Nichol, 2021]. This method uses a separately trained classifier <span class="math inline">\(f_\phi(y \vert \mathbf{x}_t)\)</span> that predicts the class label <span class="math inline">\(y\)</span> given a noisy input <span class="math inline">\(\mathbf{x}_t\)</span>. During generation, the gradient of the classifier’s log-likelihood <span class="math inline">\(\nabla_{\mathbf{x}_t} \log f_\phi(y \vert \mathbf{x}_t)\)</span> is used to “guide” the noise prediction towards the desired class. The modified noise prediction is: <span class="math display">\[\bar{\boldsymbol{\epsilon}}_\theta(\mathbf{x}_t, t) = \boldsymbol{\epsilon}_\theta(x_t, t) - w \sqrt{1 - \bar{\alpha}_t} \nabla_{\mathbf{x}_t} \log f_\phi(y \vert \mathbf{x}_t)\]</span> where <span class="math inline">\(w\)</span> is a guidance scale factor. This was used in models like ADM (Ablated Diffusion Model) and ADM-G (ADM with Guidance).</p></li>
<li><p><strong>Classifier-Free Guidance (CFG):</strong> Proposed by [Ho &amp; Salimans, 2021]. This popular technique avoids the need for a separate classifier. The diffusion model <span class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span> itself is trained to handle both conditional inputs (<span class="math inline">\(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, y)\)</span>) and unconditional inputs (<span class="math inline">\(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, \varnothing)\)</span>, where <span class="math inline">\(y=\varnothing\)</span> represents the null condition). This is often achieved by randomly dropping the condition <span class="math inline">\(y\)</span> during training. At inference time, guidance is achieved by extrapolating from the conditional and unconditional predictions: <span class="math display">\[\bar{\boldsymbol{\epsilon}}_\theta(\mathbf{x}_t, t, y) = \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, \varnothing) + w (\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, y) - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, \varnothing))\]</span> This can also be written as <span class="math inline">\((w+1) \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, y) - w \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, \varnothing)\)</span>. CFG is widely used in modern high-performance models like Imagen, Stable Diffusion, and GLIDE. [Nichol et al., 2022] (GLIDE) found CFG to yield better results than guidance based on CLIP embeddings.</p></li>
</ul>
</section>
<section id="speeding-up-sampling" class="level3">
<h3 class="anchored" data-anchor-id="speeding-up-sampling">Speeding Up Sampling</h3>
<p>The primary drawback of early diffusion models was the slow sampling speed due to the large number of steps (<span class="math inline">\(T\)</span>). Significant progress has been made here.</p>
<ul>
<li><p><strong>DDIM (Denoising Diffusion Implicit Models):</strong> Proposed by [Song et al., 2020]. While sharing the same forward process as DDPM, DDIM defines a non-Markovian <em>generative</em> process that allows for much larger step sizes. This makes the sampling deterministic (controlled by a parameter <span class="math inline">\(\eta\)</span>; <span class="math inline">\(\eta=0\)</span> for DDIM, <span class="math inline">\(\eta=1\)</span> approximates DDPM) and significantly faster (e.g., reducing 1000 steps to 20-50) while maintaining high sample quality. Its deterministic nature also ensures “consistency” (same noise yields same image) and enables latent space interpolation.</p></li>
<li><p><strong>Progressive Distillation:</strong> Proposed by [Salimans &amp; Ho, 2022]. This technique distills a trained deterministic sampler (like DDIM) into a new “student” model that takes half the number of steps. The student learns to perform two steps of the “teacher” model in a single step. This process can be repeated, exponentially reducing sampling time.</p></li>
<li><p><strong>Consistency Models:</strong> Proposed by [Song et al., 2023]. These models learn a function <span class="math inline">\(f(\mathbf{x}_t, t) \approx \mathbf{x}_0\)</span> that directly maps any point <span class="math inline">\(\mathbf{x}_t\)</span> on a diffusion trajectory back to its origin (or near-origin <span class="math inline">\(\mathbf{x}_\epsilon\)</span>). They possess a “self-consistency” property. They can be trained either by distilling a pre-trained diffusion model (Consistency Distillation, CD) or from scratch (Consistency Training, CT). They hold the potential for high-quality generation in very few steps, even just one.</p></li>
<li><p><strong>Latent Diffusion Models (LDM):</strong> Proposed by [Rombach et al., 2022]. Instead of operating directly in the high-dimensional pixel space, LDMs first use a powerful autoencoder (Encoder <span class="math inline">\(\mathcal{E}\)</span>, Decoder <span class="math inline">\(\mathcal{D}\)</span>) to compress the image <span class="math inline">\(\mathbf{x}\)</span> into a lower-dimensional latent representation <span class="math inline">\(\mathbf{z} = \mathcal{E}(\mathbf{x})\)</span>. The diffusion process (often using a U-Net) is then applied entirely within this latent space. To generate an image, noise is denoised in the latent space to produce <span class="math inline">\(\mathbf{z}\)</span>, which is then mapped back to the pixel space using the decoder <span class="math inline">\(\mathcal{D}(\mathbf{z})\)</span>. This drastically reduces computational cost and memory requirements, forming the basis for models like Stable Diffusion. Regularization techniques like KL penalty or Vector Quantization (VQ) are used in the autoencoder training. Conditioning is often implemented using cross-attention mechanisms within the latent U-Net.</p></li>
</ul>
</section>
<section id="achieving-higher-resolution-and-quality" class="level3">
<h3 class="anchored" data-anchor-id="achieving-higher-resolution-and-quality">Achieving Higher Resolution and Quality</h3>
<ul>
<li><p><strong>Cascaded Models:</strong> Employed by [Ho et al., 2021] and others. This involves a pipeline approach: first generate a low-resolution image, then use one or more super-resolution diffusion models conditioned on the low-resolution output to generate progressively higher-resolution images. “Noise Conditioning Augmentation” (adding noise to the low-resolution conditioning input) was found crucial for improving quality by mitigating error accumulation.</p></li>
<li><p><strong>unCLIP / DALL-E 2:</strong> Proposed by [Ramesh et al., 2022]. These models leverage the powerful CLIP model for high-quality text-to-image generation. They typically involve a two-stage process: (1) A “prior” model generates a CLIP image embedding <span class="math inline">\(\mathbf{c}^i\)</span> conditioned on the input text <span class="math inline">\(y\)</span> (<span class="math inline">\(P(\mathbf{c}^i \vert y)\)</span>). (2) A “decoder” diffusion model generates the final image <span class="math inline">\(\mathbf{x}\)</span> conditioned on the image embedding <span class="math inline">\(\mathbf{c}^i\)</span> (and optionally the text <span class="math inline">\(y\)</span>) (<span class="math inline">\(P(\mathbf{x} \vert \mathbf{c}^i, [y])\)</span>).</p></li>
<li><p><strong>Imagen:</strong> Proposed by [Saharia et al., 2022]. Instead of CLIP, Imagen uses large, pre-trained <em>frozen</em> language models (like T5-XXL) as text encoders, finding that the scale of the text encoder was more critical than the scale of the diffusion U-Net. It introduced “Dynamic Thresholding” to improve image fidelity at high CFG scales by adaptively clipping predicted pixel values. It also proposed an “Efficient U-Net” architecture with modifications like shifting parameters to lower-resolution blocks and optimizing the order of up/downsampling operations.</p></li>
<li><p><strong>Architectural Evolution (U-Net, DiT, ControlNet):</strong></p>
<ul>
<li><em>U-Net:</em> The classic architecture with downsampling/upsampling paths and skip connections remains a standard backbone for many diffusion models, especially in image domains.</li>
<li><em>DiT (Diffusion Transformer):</em> Proposed by [Peebles &amp; Xie, 2023]. Adapts the Transformer architecture for diffusion, operating on <em>latent</em> patches (similar to LDM). It involves patchifying the latent representation, processing the sequence of patches through Transformer blocks, and incorporating conditioning (like timestep <span class="math inline">\(t\)</span> and class <span class="math inline">\(c\)</span>) via adaptive layer normalization (adaLN-Zero). DiT benefits from the known scalability of Transformers.</li>
<li><em>ControlNet:</em> Proposed by [Zhang et al., 2023]. A technique for adding fine-grained spatial control (e.g., based on edge maps, human poses, depth maps) to large, pre-trained text-to-image diffusion models <em>without</em> expensive retraining. It works by creating a trainable copy of the model’s weights and connecting it to the original frozen weights via special “zero convolution” layers. These zero-initialized layers allow stable training of the control mechanism while preserving the original model’s capabilities. The output combines the original block’s output with the controlled copy’s output: <span class="math inline">\(\mathbf{y}_c = \mathcal{F}_\theta(\mathbf{x}) + \mathcal{Z}_{\theta_{z2}}(\mathcal{F}_{\theta_c}(\mathbf{x} + \mathcal{Z}_{\theta_{z1}}(\mathbf{c})))\)</span>.</li>
</ul></li>
</ul>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>Diffusion models represent a powerful and flexible class of generative models based on systematically destroying data structure with noise and then learning to reverse the process.</p>
<ul>
<li><strong>Advantages:</strong> They achieve state-of-the-art results in generating high-quality, diverse samples, particularly for images. They benefit from theoretical tractability and relatively stable training compared to alternatives like GANs.</li>
<li><strong>Disadvantages:</strong> Historically, their main drawback was slow sampling speed, requiring many sequential denoising steps. However, significant advancements (DDIM, LDM, distillation, consistency models) have drastically improved sampling efficiency, making them much more practical.</li>
</ul>
<p>Fueled by innovations like Classifier-Free Guidance, Latent Diffusion, Transformer-based architectures, and control mechanisms like ControlNet, diffusion models are at the forefront of generative AI, enabling cutting-edge applications in text-to-image synthesis, image editing, video generation, and beyond.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ol type="1">
<li>Weng, Lilian. (Jul 2021). What are diffusion models? Lil’Log. <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</a></li>
<li>Ho, Jonathan, Ajay Jain, and Pieter Abbeel. “<a href="https://arxiv.org/abs/2006.11239">Denoising diffusion probabilistic models</a>.” NeurIPS 2020. (DDPM)</li>
<li>Song, Jiaming, Chenlin Meng, and Stefano Ermon. “<a href="https://arxiv.org/abs/2010.02502">Denoising diffusion implicit models</a>.” ICLR 2021. (DDIM)</li>
<li>Rombach, Robin, et al.&nbsp;“<a href="https://arxiv.org/abs/2112.10752">High-resolution image synthesis with latent diffusion models</a>.” CVPR 2022. (Latent Diffusion / Foundation for Stable Diffusion)</li>
<li>Nichol, Alex, and Prafulla Dhariwal. “<a href="https://arxiv.org/abs/2102.09672">Improved denoising diffusion probabilistic models</a>.” ICML 2021.</li>
<li>Dhariwal, Prafulla, and Alex Nichol. “<a href="https://arxiv.org/abs/2105.05233">Diffusion models beat gans on image synthesis</a>.” NeurIPS 2021.</li>
<li>Ho, Jonathan, and Tim Salimans. “<a href="https://arxiv.org/abs/2207.12598">Classifier-free diffusion guidance</a>.” NeurIPS 2021 Workshop.</li>
<li>Salimans, Tim, and Jonathan Ho. “<a href="https://arxiv.org/abs/2202.00512">Progressive distillation for fast sampling of diffusion models</a>.” ICLR 2022.</li>
<li>Song, Yang, et al.&nbsp;“<a href="https://arxiv.org/abs/2303.01469">Consistency models</a>.” ICML 2023.</li>
<li>Ho, Jonathan, et al.&nbsp;“<a href="https://arxiv.org/abs/2106.15282">Cascaded diffusion models for high fidelity image generation</a>.” JMLR 2022.</li>
<li>Ramesh, Aditya, et al.&nbsp;“<a href="https://arxiv.org/abs/2204.06125">Hierarchical text-conditional image generation with clip latents</a>.” arXiv 2022. (unCLIP / DALL-E 2)</li>
<li>Saharia, Chitwan, et al.&nbsp;“<a href="https://arxiv.org/abs/2205.11487">Photorealistic text-to-image diffusion models with deep language understanding</a>.” NeurIPS 2022. (Imagen)</li>
<li>Peebles, William, and Saining Xie. “<a href="https://arxiv.org/abs/2212.09748">Scalable diffusion models with transformers</a>.” ICCV 2023. (DiT)</li>
<li>Zhang, Lvmin, and Maneesh Agrawala. “<a href="https://arxiv.org/abs/2302.05543">Adding conditional control to text-to-image diffusion models</a>.” ICCV 2023. (ControlNet)</li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/jiwasawa\.github\.io\/blog\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>