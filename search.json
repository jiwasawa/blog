[
  {
    "objectID": "posts/transformer-attention/index.html",
    "href": "posts/transformer-attention/index.html",
    "title": "Understanding Self-Attention",
    "section": "",
    "text": "Lately, Large Language Models (LLMs) like ChatGPT and GPT-4 have taken the world by storm. These models demonstrate remarkable abilities, from generating code and drafting emails to answering complex questions and even writing creative prose. At the heart of many of these systems lies the Transformer architecture, introduced in the groundbreaking 2017 paper, “Attention is All You Need”.\nBut what exactly is this “Attention” mechanism, and how does it empower models like GPT to understand context and generate coherent text?\nAndrej Karpathy’s excellent video, “Let’s build GPT: from scratch, in code, spelled out.”, demystifies the Transformer by building a small version from the ground up, which he calls nanogpt. Let’s follow his lead and unravel the workings of self-attention, the core engine of the Transformer."
  },
  {
    "objectID": "posts/transformer-attention/index.html#getting-started-the-basics-of-language-modeling",
    "href": "posts/transformer-attention/index.html#getting-started-the-basics-of-language-modeling",
    "title": "Understanding Self-Attention",
    "section": "Getting Started: The Basics of Language Modeling",
    "text": "Getting Started: The Basics of Language Modeling\nBefore diving into attention, let’s grasp the fundamental task: language modeling. The goal of language modeling is to predict the next word (or character, or token) in a sequence, given the preceding sequence (the context).\nKarpathy starts with the “Tiny Shakespeare” dataset – a single text file containing concatenated works of Shakespeare.\n# First, let's prepare our training dataset. We'll download the Tiny Shakespeare dataset.\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n# Let's read it to see what's inside.\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# Let's list all the unique characters that occur in this text.\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\n# !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nprint(vocab_size)\n# 65\n\n# Create a mapping from characters to integers.\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\nprint(encode(\"hii there\"))\n# [46, 47, 47, 1, 58, 46, 43, 56, 43]\nprint(decode(encode(\"hii there\")))\n# hii there\n# Now let's encode the entire text dataset and store it into a torch.Tensor.\nimport torch # We use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\n# torch.Size([1115394]) torch.int64\nprint(data[:1000]) # The first 1,000 characters we looked at earlier will look like this to the GPT\n# tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44, ...\nIn this example, the text is tokenized at the character level, mapping each character to a number. The model’s job is to predict the next number in the sequence, given a sequence of numbers.\nKarpathy first implements the simplest possible language model: the Bigram Model.\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        # Later in the video, this changes to vocab_size x n_embd\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        # In the Bigram model, logits are looked up directly\n        logits = self.token_embedding_table(idx) # (B,T,C) where initially C=vocab_size\n\n        if targets is None:\n            loss = None\n        else:\n            # reshape for cross_entropy\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nm = BigramLanguageModel(vocab_size)\n# Assuming xb, yb are batches from get_batch function (not shown here, but in Karpathy's code)\n# logits, loss = m(xb, yb) \n# print(logits.shape) # Example shape if B=4, T=8 -&gt; (32, 65) -&gt; after view (B*T, C)\n# print(loss) # Example loss tensor\n\n# Generate text (assuming 'idx' starts as torch.zeros((1, 1), dtype=torch.long))\n# print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n# Example output (untrained): SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGpwnYWmnxKWWev-tDqXErVKLgJ\nLet’s train this simple model.\n# Create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nbatch_size = 32 # How many independent sequences will we process in parallel?\n# Assuming get_batch function exists as in Karpathy's code\n# def get_batch(split): ... return xb, yb\n\nfor steps in range(10000): # increase number of steps for good results...\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# print(loss.item()) # Example loss after some training\n# print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n# Example output (after some training): oTo.JUZ!!zqe!\n# xBP qbs$Gy'AcOmrLwwt ... (still mostly nonsense)\nThis model uses an embedding table where the index of the input character directly looks up a probability distribution (logits) for the next character. It’s simple, but has a critical flaw: it completely ignores context. The prediction after ‘t’ in “hat” is the same as after ‘t’ in “bat”. The tokens aren’t “talking” to each other."
  },
  {
    "objectID": "posts/transformer-attention/index.html#the-need-for-communication-aggregating-past-information",
    "href": "posts/transformer-attention/index.html#the-need-for-communication-aggregating-past-information",
    "title": "Understanding Self-Attention",
    "section": "The Need for Communication: Aggregating Past Information",
    "text": "The Need for Communication: Aggregating Past Information\nTo make better predictions, tokens need information from the tokens that came before them in the sequence. How can tokens communicate?\nKarpathy introduces a “mathematical trick” using matrix multiplication. The simplest way for a token to get context is to average the information from all preceding tokens, including itself.\nSuppose our input x has shape (B, T, C) (Batch, Time (sequence length), Channels (embedding dimension)). We want to compute xbow (a bag-of-words representation) such that xbow[b, t] contains the average of x[b, 0] through x[b, t].\nA simple loop is inefficient:\n# We want xbow[b,t] = mean_{i&lt;=t} x[b,i]\n# (assuming x is defined with shape B, T, C)\nB,T,C = 4,8,32 # example dimensions\nx = torch.randn(B,T,C)\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t+1, C)\n        xbow[b,t] = torch.mean(xprev, 0)\nA much more efficient way uses matrix multiplication with a lower-triangular matrix:\n# version 2: using matrix multiply for a weighted aggregation\nT = 8 # Example sequence length\n# toy example illustrating how matrix multiplication can be used for weighted aggregation.\nwei = torch.tril(torch.ones(T, T)) # Lower-triangular matrix of ones\nwei = wei / wei.sum(1, keepdim=True) # Normalize rows to sum to 1 -&gt; averaging\n# Example x with B=4, T=8, C=32\nx = torch.randn(4, T, 32)\nxbow2 = wei @ x # (T, T) @ (B, T, C) ----&gt; (B, T, C) due to broadcasting\ntorch.allclose(xbow, xbow2) # True\nHere, wei (weights) is a (T, T) matrix. Row t of wei has non-zero values (in this case, 1/(t+1)) only in columns 0 through t. Multiplying this with x (shape (B, T, C)), PyTorch broadcasts wei across the batch dimension. The resulting xbow2[b, t] becomes the weighted sum (average, in this case) of x[b, 0] through x[b, t].\nThis matrix multiplication efficiently performs the aggregation. We can also achieve this using softmax:\n# version 3: use Softmax\nT = 8\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf')) # Fill upper triangle with -inf\nwei = F.softmax(wei, dim=-1) # Softmax makes rows sum to 1, recovering averaging weights\nxbow3 = wei @ x\n# torch.allclose(xbow, xbow3) should be True\nWhy use softmax here? It introduces the crucial idea that the weights (wei) don’t have to be a fixed average; they can be learned or data-dependent. This is exactly what self-attention does."
  },
  {
    "objectID": "posts/transformer-attention/index.html#introducing-positional-information-position-encoding",
    "href": "posts/transformer-attention/index.html#introducing-positional-information-position-encoding",
    "title": "Understanding Self-Attention",
    "section": "Introducing Positional Information: Position Encoding",
    "text": "Introducing Positional Information: Position Encoding\nBefore diving into the self-attention mechanism itself, there’s another crucial element: information about the token’s position in the sequence.\nThe basic self-attention calculation (weighted aggregation using Query, Key, and Value) doesn’t inherently consider where tokens are located. If you shuffled the words in a sentence, the attention scores between any two given words (based on their vectors alone) wouldn’t change. This is problematic, as word order is fundamental to meaning. “The cat sat on the mat” means something very different from “The mat sat on the cat.”\nTo solve this, Transformers add a Position Encoding vector to the token’s own embedding vector (Token Embedding). This combined vector represents both the token’s meaning and its position.\nIn Karpathy’s nanogpt, learnable position encodings are used. Specifically, an embedding table (position_embedding_table) stores position vectors for up to the maximum sequence length (block_size). For a sequence of length T, integers from 0 to T-1 are used as indices to retrieve the corresponding position vectors from this table.\n# Excerpt from the forward method in BigramLanguageModel (or later GPTLanguageModel)\n# Assuming idx is the input tensor of token indices (B, T)\n# Assuming self.token_embedding_table and self.position_embedding_table are defined\n# Assuming n_embd is the embedding dimension C\n# Assuming block_size is the maximum context length\n# Assuming device is set ('cuda' or 'cpu')\n\nB, T = idx.shape\n\n# idx and targets are both (B,T) tensor of integers\ntok_emb = self.token_embedding_table(idx) # (B,T,C) - Token embeddings\n# torch.arange(T, device=device) generates integer sequence 0, 1, ..., T-1\npos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C) - Position embeddings\nx = tok_emb + pos_emb # (B,T,C) - Add token and position embeddings\n# x = self.blocks(x) # ... this x becomes the input to the Transformer blocks ...\nThis creates the vector x, containing both the token’s identity (tok_emb) and its position (pos_emb). This x is the actual input passed into the subsequent Transformer blocks (Self-Attention and FeedForward layers), allowing the model to consider both meaning and order."
  },
  {
    "objectID": "posts/transformer-attention/index.html#self-attention-data-dependent-information-aggregation",
    "href": "posts/transformer-attention/index.html#self-attention-data-dependent-information-aggregation",
    "title": "Understanding Self-Attention",
    "section": "Self-Attention: Data-Dependent Information Aggregation",
    "text": "Self-Attention: Data-Dependent Information Aggregation\nSimple averaging treats all past tokens equally. But often, some past tokens are much more relevant than others. When predicting the word after “The cat sat on the…”, the word “cat” is likely more important than “The”.\nSelf-attention allows tokens to query other tokens and assign attention scores based on relevance. Each token produces three vectors:\n\nQuery (Q): What am I looking for?\nKey (K): What information do I possess?\nValue (V): If attention is paid to me, what information will I provide?\n\nThe attention score (or affinity) between token i and token j is calculated by taking the dot product of token i’s Query vector (q_i) and token j’s Key vector (k_j):\naffinity(i, j) = q_i ⋅ k_j\nA high dot product means the Query matches the Key well, indicating token j is relevant to token i.\nHere’s how a single “Head” of attention is implemented:\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels (embedding dimension)\nx = torch.randn(B,T,C) # Input token embeddings + position encodings\n\n# Let's see a single Head perform self-attention\nhead_size = 16 # Dimension of K, Q, V vectors for this head\n# Linear layers to project input 'x' into K, Q, V\nkey   = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\n\nk = key(x)   # (B, T, head_size)\nq = query(x) # (B, T, head_size)\n\n# compute attention scores (\"affinities\")\n# (B, T, head_size) @ (B, head_size, T) ---&gt; (B, T, T)\nwei =  q @ k.transpose(-2, -1) # wei means \"weights\" or scores\n\n# --- Scaling Step (discussed below) ---\n# Scale the affinities\nwei = wei * (head_size**-0.5) \n\n# --- Masking for Decoder ---\n# Assume T is the sequence length (e.g., 8 here)\n# Assume x.device holds the correct device ('cuda' or 'cpu')\ntril = torch.tril(torch.ones(T, T, device=x.device)) \n# Mask out future tokens\nwei = wei.masked_fill(tril == 0, float('-inf')) \n\n# --- Normalize Scores to Probabilities ---\nwei = F.softmax(wei, dim=-1) # (B, T, T)\n\n# --- Perform the weighted aggregation of Values ---\nv = value(x) # (B, T, head_size)\n# (B, T, T) @ (B, T, head_size) ---&gt; (B, T, head_size)\nout = wei @ v\n\n# out.shape is (B, T, head_size)\nLet’s break down the key steps:\n\nProjection: The input x (containing token + position info) is projected into K, Q, and V spaces using linear layers.\nAffinity Calculation: q @ k.transpose(...) computes the dot product between every pair of Query and Key vectors within each sequence in the batch. This yields wei, the raw attention scores (shape B, T, T).\nScaling: The scores wei are scaled down by the square root of head_size. This is crucial for stabilizing training, especially during initialization. Without scaling, the variance of the dot products grows with head_size, potentially pushing the inputs to softmax into regions with tiny gradients, hindering learning. This is the “Scaled” part of “Scaled Dot-Product Attention”.\nMasking (Decoder-Specific): In autoregressive language modeling like GPT, a token at position t should only attend to tokens up to position t. This is achieved by setting the attention scores corresponding to future positions (j &gt; t) to negative infinity using masked_fill with a lower-triangular matrix (tril). Softmax then assigns zero probability to these future tokens. (Note: Encoder blocks, like in BERT, do not use this causal mask).\nSoftmax: Softmax is applied row-wise to the masked scores. This converts the scores into probabilities that sum to 1 for each token t, representing the attention distribution over preceding tokens 0 to t.\nValue Aggregation: The final output out for each token t is a weighted sum of the Value vectors (v) of all tokens, weighted by the attention probabilities in wei. out = wei @ v.\n\nThe output out (shape B, T, head_size) contains, for each token, aggregated information from other relevant tokens in the sequence, based on the learned K, Q, V projections."
  },
  {
    "objectID": "posts/transformer-attention/index.html#multi-head-attention-multiple-perspectives",
    "href": "posts/transformer-attention/index.html#multi-head-attention-multiple-perspectives",
    "title": "Understanding Self-Attention",
    "section": "Multi-Head Attention: Multiple Perspectives",
    "text": "Multi-Head Attention: Multiple Perspectives\nA single attention head might learn to focus on a specific type of relationship (e.g., noun-verb agreement). To capture diverse relationships, Transformers use Multi-Head Attention.\n# Assuming n_embd, block_size, dropout are defined hyperparameters\n# n_embd = 384 # Example embedding dimension\n# block_size = 256 # Example max context length\n# dropout = 0.2 # Example dropout rate\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        # tril is registered as a buffer (not a parameter)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout) # Add dropout\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,head_size)\n        q = self.query(x) # (B,T,head_size)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # scale by head_size**-0.5\n        # Apply mask dynamically based on T\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Use only up to T x T part of tril\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei) # Apply dropout to attention weights\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,head_size)\n        out = wei @ v # (B,T,head_size)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        # Create multiple Head instances\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        # Projection layer after concatenation\n        self.proj = nn.Linear(num_heads * head_size, n_embd) # n_embd = num_heads * head_size assumed\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # Run each head in parallel and concatenate results along the channel dimension\n        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, num_heads * head_size)\n        # Re-project the concatenated output back to the original n_embd dimension\n        out = self.dropout(self.proj(out)) # (B, T, n_embd)\n        return out\nThis simply runs multiple Head modules in parallel, potentially each with different learned K, Q, V projections. The outputs of each head (each B, T, head_size) are concatenated (B, T, num_heads * head_size) and then projected back to the original embedding dimension (B, T, n_embd) using another linear layer (self.proj). This allows the model to simultaneously attend to information from different representation subspaces."
  },
  {
    "objectID": "posts/transformer-attention/index.html#attention-flavors-self-cross-encoders-decoders",
    "href": "posts/transformer-attention/index.html#attention-flavors-self-cross-encoders-decoders",
    "title": "Understanding Self-Attention",
    "section": "Attention Flavors: Self, Cross, Encoders & Decoders",
    "text": "Attention Flavors: Self, Cross, Encoders & Decoders\nThe basic mechanism we’ve discussed so far is often called Self-Attention because the Query (Q), Key (K), and Value (V) vectors are all derived from the same input sequence (x), allowing tokens within that sequence to attend to each other. However, there are important variations in how self-attention is used and in the broader attention mechanism.\nFirstly, how self-attention is used differs between Encoder and Decoder blocks, primarily due to masking.\nSelf-attention in a Decoder block employs causal masking (the triangular mask) to prevent tokens from attending to future positions. This is essential for autoregressive models like GPT or the decoder part of a machine translation model, where generation must rely only on past information. Karpathy’s nanogpt is precisely a model composed only of these Decoder blocks.\nConversely, self-attention in an Encoder block does not use causal masking. All tokens in the sequence can freely attend to all other tokens (past and future). This is used in models like BERT, which aim to understand the full context of an input text, or in the encoder part of a machine translation model (which encodes the entire source sentence). It’s suited for capturing bidirectional context.\nSecondly, another crucial form of attention is Cross-Attention. Unlike self-attention (masked or unmasked), the sources for Query, Key, and Value differ. In cross-attention, the Query (Q) typically comes from one source (e.g., the decoder’s state), while the Key (K) and Value (V) come from another source (e.g., the final output of the encoder).\nCross-attention primarily serves to connect the Encoder and Decoder in an Encoder-Decoder architecture. It allows the decoder, as it generates output tokens, to continually refer back to the entire encoded input information via the K and V vectors from the encoder. This enables tasks like machine translation, where the model needs to consider the meaning of the source sentence while generating the target language.\nSince nanogpt is a decoder-only model, it doesn’t have an encoder to process an external input sequence. Therefore, it doesn’t need Encoder blocks or Cross-Attention; it consists solely of Self-Attention with causal masking (Decoder blocks)."
  },
  {
    "objectID": "posts/transformer-attention/index.html#the-transformer-block-communication-and-computation",
    "href": "posts/transformer-attention/index.html#the-transformer-block-communication-and-computation",
    "title": "Understanding Self-Attention",
    "section": "The Transformer Block: Communication and Computation",
    "text": "The Transformer Block: Communication and Computation\nAttention provides the communication mechanism. But the model also needs computation to process the aggregated information. A standard Transformer block combines Multi-Head Self-Attention with a simple, position-wise FeedForward network.\nCrucially, Residual Connections and Layer Normalization are added around each sub-layer (Attention and FeedForward).\n\nResidual Connections: x = x + sublayer(norm(x)). The input x to the sub-layer is added to the output of the sub-layer. This significantly helps gradients flow during backpropagation in deep networks, stabilizing training and improving performance.\nLayer Normalization: Normalizes the features independently for each token across the channel dimension. Unlike Batch Normalization, it doesn’t rely on batch statistics, making it well-suited for sequence data. It also stabilizes training. Karpathy implements the common “pre-norm” formulation, where LayerNorm is applied before the sub-layer.\n\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd), # Inner layer is typically 4x larger\n            nn.ReLU(),                     # ReLU activation\n            nn.Linear(4 * n_embd, n_embd), # Project back to n_embd\n            nn.Dropout(dropout),            # Dropout for regularization\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size) # Communication\n        self.ffwd = FeedFoward(n_embd)                  # Computation\n        self.ln1 = nn.LayerNorm(n_embd)                 # LayerNorm before Attention\n        self.ln2 = nn.LayerNorm(n_embd)                 # LayerNorm before FeedForward\n\n    def forward(self, x):\n        # Pre-norm formulation with residual connections\n        # Apply LayerNorm -&gt; Self-Attention -&gt; Add residual\n        x = x + self.sa(self.ln1(x))\n        # Apply LayerNorm -&gt; FeedForward -&gt; Add residual\n        x = x + self.ffwd(self.ln2(x))\n        return x\nA full GPT model simply stacks multiple of these Block layers sequentially. After passing through all blocks, a final LayerNorm is applied, followed by a linear layer that projects the final token representations to the vocabulary size, yielding logits for predicting the next token."
  },
  {
    "objectID": "posts/transformer-attention/index.html#putting-it-all-together-the-final-gpt-model",
    "href": "posts/transformer-attention/index.html#putting-it-all-together-the-final-gpt-model",
    "title": "Understanding Self-Attention",
    "section": "Putting it all Together: The Final GPT Model",
    "text": "Putting it all Together: The Final GPT Model\nIntegrating the components discussed, we arrive at the final GPT-style language model, GPTLanguageModel. The code below represents the completed version from Karpathy’s video, incorporating the Block (which includes MultiHeadAttention and FeedForward) and other elements.\n# (Reiterating key hyperparameters)\n# hyperparameters\nbatch_size = 64 # how many independent sequences will we process in parallel?\nblock_size = 256 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384 # embedding dimension\nn_head = 6   # number of attention heads\nn_layer = 6  # number of Transformer blocks (layers)\ndropout = 0.2 # dropout rate\n# ------------\n\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        # Stack n_layer Transformer blocks\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size) # output layer (linear)\n\n        # Better weight initialization (important but not covered in detail in the video walk-through)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        # (Weight initialization details omitted for brevity, see Karpathy's code)\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C) Pass through Transformer blocks\n        x = self.ln_f(x) # (B,T,C) Apply final LayerNorm\n        logits = self.lm_head(x) # (B,T,vocab_size) Compute logits via LM head\n\n        if targets is None:\n            loss = None\n        else:\n            # Reshape for loss calculation\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens due to position embedding size limit\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond) # perform forward pass\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\n# Example Usage (assuming training loop and data loading are set up)\n# model = GPTLanguageModel()\n# m = model.to(device)\n# ... training loop using optimizer and get_batch ...\n# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n# print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\nIn this GPTLanguageModel class, the __init__ method defines the token and position embedding tables, stacks n_layer Blocks using nn.Sequential (the core Transformer), adds a final LayerNorm (ln_f), and the output linear layer (lm_head). It also includes the _init_weights method crucial for stable training.\nThe forward method implements the flow: add token and position embeddings, pass through the blocks, apply final normalization, and project to logits.\nThe generate method produces text autoregressively. The key line idx_cond = idx[:, -block_size:] highlights a constraint: because the position_embedding_table has a fixed size (block_size), the model can only condition on the most recent block_size tokens when making a prediction. Within this context window, it performs a forward pass, samples the next token based on the final timestep’s logits, and extends the sequence.\nThe complete code also involves hyperparameters (like batch_size, learning_rate), an AdamW optimizer, and a standard training loop with evaluation (using an estimate_loss function), all working together to train and run the GPT model."
  },
  {
    "objectID": "posts/transformer-attention/index.html#scaling-up-and-results",
    "href": "posts/transformer-attention/index.html#scaling-up-and-results",
    "title": "Understanding Self-Attention",
    "section": "Scaling Up and Results",
    "text": "Scaling Up and Results\nKarpathy trains this GPTLanguageModel (with n_layer=6, n_head=6, n_embd=384, dropout=0.2) on Tiny Shakespeare. The resulting model generates much more coherent (though still nonsensical) Shakespeare-like text, demonstrating the power of attention combined with sufficient model capacity.\n# Sample output from the trained GPTLanguageModel\nFlY BOLINGLO:\nThem thrumply towiter arts the\nmuscue rike begatt the sea it\nWhat satell in rowers that some than othis Marrity.\n\nLUCENTVO:\nBut userman these that, where can is not diesty rege;\nWhat and see to not. But's eyes. What?\nThis architecture—the decoder-only Transformer (using causal masking)—is fundamentally the same as that used in models like GPT-2 and GPT-3, just massively scaled up in terms of the number of parameters, layers, embedding sizes, and, crucially, the training data (vast amounts of internet text instead of just Shakespeare)."
  },
  {
    "objectID": "posts/transformer-attention/index.html#conclusion",
    "href": "posts/transformer-attention/index.html#conclusion",
    "title": "Understanding Self-Attention",
    "section": "Conclusion",
    "text": "Conclusion\nThe attention mechanism, specifically scaled dot-product self-attention, is the innovation that unlocked the power of Transformers. It allows tokens in a sequence to dynamically query each other, compute relevance scores (affinities) based on learned Query-Key interactions, and aggregate information from relevant tokens’ Value vectors in a weighted manner. Combined with Multi-Head Attention, Residual Connections, Layer Normalization, and position-wise FeedForward networks, it forms the Transformer block – the fundamental building block of the models revolutionizing AI today.\nBy building it up step-by-step, as Karpathy demonstrates, we see that while powerful, the core ideas are graspable and can be implemented with relatively concise code.\n\nThis post is based on Andrej Karpathy’s YouTube video “Let’s build GPT: from scratch, in code, spelled out.”. For the complete code and deeper insights, definitely check out the video and his nanogpt repository. Hopefully, this walkthrough helps clarify the magic behind Transformers and Attention!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Understanding Self-Attention\n\n\n\n\n\n\nMachine Learning\n\n\nTransformer\n\n\nPython\n\n\nLLM\n\n\n\n\n\n\n\n\n\nApr 11, 2025\n\n\nJunichiro Iwasawa\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Junichiro Iwasawa",
    "section": "",
    "text": "Personal webpage: jiwasawa.github.io/"
  }
]