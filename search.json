[
  {
    "objectID": "posts/difusion-basics/index.html",
    "href": "posts/difusion-basics/index.html",
    "title": "Introduction to Diffusion Models: From Core Concepts to Cutting-Edge Applications",
    "section": "",
    "text": "In recent years, diffusion models have taken the generative modeling world by storm, particularly in image synthesis, often producing stunning results. This post aims to provide a comprehensive introduction, starting from the fundamental concepts and moving towards the advanced techniques that power today’s state-of-the-art models."
  },
  {
    "objectID": "posts/difusion-basics/index.html#what-are-diffusion-models",
    "href": "posts/difusion-basics/index.html#what-are-diffusion-models",
    "title": "Introduction to Diffusion Models: From Core Concepts to Cutting-Edge Applications",
    "section": "What are Diffusion Models?",
    "text": "What are Diffusion Models?\nDiffusion models are a class of generative models. While other approaches like Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Flow-based models have achieved significant success, they each come with their own challenges—GANs can suffer from training instability, VAEs rely on surrogate loss functions, and Flow models require specialized architectures for reversible transformations.\nInspired by non-equilibrium thermodynamics, diffusion models offer a different paradigm. They work through two main processes:\n\nForward Process (Diffusion Process): Gradually add small amounts of random noise (typically Gaussian) to the data over many steps, eventually transforming the data distribution into a simple, known distribution (like a standard Gaussian).\nReverse Process (Denoising Process): Learn to reverse the diffusion process. Starting from pure noise, incrementally remove the noise step-by-step to generate a sample that belongs to the original data distribution.\n\nThe core of the generative capability lies in the neural network trained to perform this “denoising” at each step. Diffusion models feature a fixed training procedure and, unlike VAEs or Flow models, typically operate with latent variables that have the same dimensionality as the original data.\n\nThe Forward Process: Turning Data into Noise\nStarting with an initial data point \\(\\mathbf{x}_0\\) drawn from the true data distribution \\(q(\\mathbf{x})\\), the forward process defines a Markov chain that adds Gaussian noise over \\(T\\) discrete time steps. The transition at each step \\(t\\) is defined as:\n\\[q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1}, \\beta_t\\mathbf{I})\\]\nHere, \\(\\\\{\\beta_t \\in (0, 1)\\\\}_{t=1}^T\\) is a variance schedule, a set of hyperparameters controlling the amount of noise added at each step. Typically, \\(\\beta_t\\) increases with \\(t\\), meaning more noise is added in later steps (common schedules include linear and cosine [Nichol & Dhariwal, 2021]). \\(\\mathbf{I}\\) is the identity matrix.\nThe joint distribution over all noisy samples given the initial data is:\n\\[q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0) = \\prod^T_{t=1} q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})\\]\nA key property of this process is that we can sample the noisy version \\(\\mathbf{x}_t\\) at any arbitrary timestep \\(t\\) directly from the original data \\(\\mathbf{x}_0\\) in a closed form. Defining \\(\\alpha_t = 1 - \\beta_t\\) and \\(\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i\\), the distribution of \\(\\mathbf{x}_t\\) given \\(\\mathbf{x}_0\\) is:\n\\[q(\\mathbf{x}_t \\vert \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t)\\mathbf{I})\\]\nThis can also be written using the reparameterization trick: \\(\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\boldsymbol{\\epsilon}\\), where \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\) is standard Gaussian noise. Essentially, \\(\\mathbf{x}_t\\) is a scaled version of the original data plus scaled noise. As \\(T \\to \\infty\\), \\(\\bar{\\alpha}_T \\approx 0\\), and \\(\\mathbf{x}_T\\) becomes almost pure Gaussian noise \\(\\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\), independent of the starting \\(\\mathbf{x}_0\\).\n\n\nThe Reverse Process: From Noise back to Data\nThe generative process reverses the forward diffusion. We start by sampling pure noise \\(\\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\) and then iteratively sample \\(\\mathbf{x}_{t-1}\\) given \\(\\mathbf{x}_t\\) for \\(t = T, T-1, \\dots, 1\\) to eventually obtain a sample \\(\\mathbf{x}_0\\).\nTo do this, we need the reverse transition probability \\(q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)\\). However, this distribution is intractable because it depends on the entire dataset. Therefore, we approximate it using a parameterized neural network, typically denoted as \\(p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)\\).\nThe entire reverse process is defined as:\n\\[p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T) \\prod^T_{t=1} p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)\\]\nwhere the starting noise distribution is \\(p(\\mathbf{x}_T) = \\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})\\). Each reverse transition step \\(p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)\\) is usually modeled as a Gaussian distribution:\n\\[p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t))\\]\nThe goal of the model is to learn the mean \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)\\) and the covariance \\(\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\) of this reverse transition. In practice, the covariance \\(\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\) is often not learned directly but is set to a fixed diagonal matrix \\(\\sigma_t^2 \\mathbf{I}\\). Common choices for \\(\\sigma_t^2\\) include \\(\\beta_t\\) (from the forward process) or \\(\\tilde{\\beta}_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t\\) (derived theoretically). While learning the variance was explored by [Nichol & Dhariwal, 2021] (e.g., as an interpolation between \\(\\beta_t\\) and \\(\\tilde{\\beta}_t\\)), it can sometimes lead to instability.\n\n\nThe Learning Objective: Predicting the Noise\nHow do we train the network to learn \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)\\)? While the full derivation involves maximizing the Variational Lower Bound (VLB) on the data log-likelihood, the DDPM paper [Ho et al., 2020] introduced a simpler, more intuitive objective that works remarkably well in practice.\nThe core idea is to reparameterize the model. Instead of directly predicting the mean \\(\\boldsymbol{\\mu}_\\theta\\) of the reverse step, the model is trained to predict the noise component \\(\\boldsymbol{\\epsilon}\\) that was added to the original data \\(\\mathbf{x}_0\\) to produce \\(\\mathbf{x}_t\\) during the forward process. Let’s denote this noise-predicting model as \\(\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\).\nUsing the relationship \\(\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\boldsymbol{\\epsilon}\\), we can express the (true) mean of the reverse step \\(\\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0)\\) (which we could compute if we knew \\(\\mathbf{x}_0\\)) in terms of this noise \\(\\boldsymbol{\\epsilon}\\). We train our model \\(\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\) to predict this true noise \\(\\boldsymbol{\\epsilon}\\), which in turn allows us to estimate the desired mean \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)\\).\nSpecifically, the learned mean \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)\\) is parameterized using the predicted noise \\(\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\) as follows:\n\\[\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\right)\\]\nThis equation shows that learning the noise prediction model \\(\\boldsymbol{\\epsilon}_\\theta\\) is sufficient to determine the mean of the reverse step.\nThe simplified training objective proposed in DDPM is then simply to minimize the Mean Squared Error (MSE) between the predicted noise \\(\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\) and the actual noise \\(\\boldsymbol{\\epsilon}\\) that was added:\n\\[L_\\text{simple} = \\mathbb{E}_{t \\sim \\mathcal{U}(1, T), \\mathbf{x}_0 \\sim q(\\mathbf{x}_0), \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})} \\left[\\|\\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}, t)\\|^2 \\right]\\]\nTraining Loop: 1. Sample a real data point \\(\\mathbf{x}_0 \\sim q(\\mathbf{x}_0)\\). 2. Sample a random timestep \\(t\\) uniformly from \\(\\\\{1, \\dots, T\\\\}\\). 3. Sample a standard Gaussian noise \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\). 4. Compute the noisy version \\(\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}\\). 5. Feed \\(\\mathbf{x}_t\\) and \\(t\\) into the model \\(\\boldsymbol{\\epsilon}_\\theta\\) to get the noise prediction \\(\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\). 6. Calculate the MSE loss between \\(\\boldsymbol{\\epsilon}\\) and \\(\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\). 7. Update the model parameters \\(\\theta\\) using gradient descent on this loss.\nConnection to Score Matching: Interestingly, this noise prediction task is closely related to score matching. The score function of a distribution \\(q(\\mathbf{x})\\) is defined as the gradient of its log-probability density, \\(\\nabla_{\\mathbf{x}} \\log q(\\mathbf{x})\\). The predicted noise \\(\\boldsymbol{\\epsilon}_\\theta\\) is approximately proportional to the score of the noisy data distribution at time \\(t\\): \\(\\mathbf{s}_\\theta(\\mathbf{x}_t, t) \\approx \\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t) \\approx - \\frac{\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)}{\\sqrt{1 - \\bar{\\alpha}_t}}\\). This links diffusion models to score-based generative models like NCSN [Song & Ermon, 2019]."
  },
  {
    "objectID": "posts/difusion-basics/index.html#evolution-and-applications-of-diffusion-models",
    "href": "posts/difusion-basics/index.html#evolution-and-applications-of-diffusion-models",
    "title": "Introduction to Diffusion Models: From Core Concepts to Cutting-Edge Applications",
    "section": "Evolution and Applications of Diffusion Models",
    "text": "Evolution and Applications of Diffusion Models\nFollowing the success of DDPM, research has focused on improving performance, expanding capabilities, and addressing limitations.\n\nConditional Generation\nGenerating samples based on specific information like class labels, text descriptions, or other images.\n\nClassifier Guidance: Proposed by [Dhariwal & Nichol, 2021]. This method uses a separately trained classifier \\(f_\\phi(y \\vert \\mathbf{x}_t)\\) that predicts the class label \\(y\\) given a noisy input \\(\\mathbf{x}_t\\). During generation, the gradient of the classifier’s log-likelihood \\(\\nabla_{\\mathbf{x}_t} \\log f_\\phi(y \\vert \\mathbf{x}_t)\\) is used to “guide” the noise prediction towards the desired class. The modified noise prediction is: \\[\\bar{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{x}_t, t) = \\boldsymbol{\\epsilon}_\\theta(x_t, t) - w \\sqrt{1 - \\bar{\\alpha}_t} \\nabla_{\\mathbf{x}_t} \\log f_\\phi(y \\vert \\mathbf{x}_t)\\] where \\(w\\) is a guidance scale factor. This was used in models like ADM (Ablated Diffusion Model) and ADM-G (ADM with Guidance).\nClassifier-Free Guidance (CFG): Proposed by [Ho & Salimans, 2021]. This popular technique avoids the need for a separate classifier. The diffusion model \\(\\boldsymbol{\\epsilon}_\\theta\\) itself is trained to handle both conditional inputs (\\(\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t, y)\\)) and unconditional inputs (\\(\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t, \\varnothing)\\), where \\(y=\\varnothing\\) represents the null condition). This is often achieved by randomly dropping the condition \\(y\\) during training. At inference time, guidance is achieved by extrapolating from the conditional and unconditional predictions: \\[\\bar{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{x}_t, t, y) = \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t, \\varnothing) + w (\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t, y) - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t, \\varnothing))\\] This can also be written as \\((w+1) \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t, y) - w \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t, \\varnothing)\\). CFG is widely used in modern high-performance models like Imagen, Stable Diffusion, and GLIDE. [Nichol et al., 2022] (GLIDE) found CFG to yield better results than guidance based on CLIP embeddings.\n\n\n\nSpeeding Up Sampling\nThe primary drawback of early diffusion models was the slow sampling speed due to the large number of steps (\\(T\\)). Significant progress has been made here.\n\nDDIM (Denoising Diffusion Implicit Models): Proposed by [Song et al., 2020]. While sharing the same forward process as DDPM, DDIM defines a non-Markovian generative process that allows for much larger step sizes. This makes the sampling deterministic (controlled by a parameter \\(\\eta\\); \\(\\eta=0\\) for DDIM, \\(\\eta=1\\) approximates DDPM) and significantly faster (e.g., reducing 1000 steps to 20-50) while maintaining high sample quality. Its deterministic nature also ensures “consistency” (same noise yields same image) and enables latent space interpolation.\nProgressive Distillation: Proposed by [Salimans & Ho, 2022]. This technique distills a trained deterministic sampler (like DDIM) into a new “student” model that takes half the number of steps. The student learns to perform two steps of the “teacher” model in a single step. This process can be repeated, exponentially reducing sampling time.\nConsistency Models: Proposed by [Song et al., 2023]. These models learn a function \\(f(\\mathbf{x}_t, t) \\approx \\mathbf{x}_0\\) that directly maps any point \\(\\mathbf{x}_t\\) on a diffusion trajectory back to its origin (or near-origin \\(\\mathbf{x}_\\epsilon\\)). They possess a “self-consistency” property. They can be trained either by distilling a pre-trained diffusion model (Consistency Distillation, CD) or from scratch (Consistency Training, CT). They hold the potential for high-quality generation in very few steps, even just one.\nLatent Diffusion Models (LDM): Proposed by [Rombach et al., 2022]. Instead of operating directly in the high-dimensional pixel space, LDMs first use a powerful autoencoder (Encoder \\(\\mathcal{E}\\), Decoder \\(\\mathcal{D}\\)) to compress the image \\(\\mathbf{x}\\) into a lower-dimensional latent representation \\(\\mathbf{z} = \\mathcal{E}(\\mathbf{x})\\). The diffusion process (often using a U-Net) is then applied entirely within this latent space. To generate an image, noise is denoised in the latent space to produce \\(\\mathbf{z}\\), which is then mapped back to the pixel space using the decoder \\(\\mathcal{D}(\\mathbf{z})\\). This drastically reduces computational cost and memory requirements, forming the basis for models like Stable Diffusion. Regularization techniques like KL penalty or Vector Quantization (VQ) are used in the autoencoder training. Conditioning is often implemented using cross-attention mechanisms within the latent U-Net.\n\n\n\nAchieving Higher Resolution and Quality\n\nCascaded Models: Employed by [Ho et al., 2021] and others. This involves a pipeline approach: first generate a low-resolution image, then use one or more super-resolution diffusion models conditioned on the low-resolution output to generate progressively higher-resolution images. “Noise Conditioning Augmentation” (adding noise to the low-resolution conditioning input) was found crucial for improving quality by mitigating error accumulation.\nunCLIP / DALL-E 2: Proposed by [Ramesh et al., 2022]. These models leverage the powerful CLIP model for high-quality text-to-image generation. They typically involve a two-stage process: (1) A “prior” model generates a CLIP image embedding \\(\\mathbf{c}^i\\) conditioned on the input text \\(y\\) (\\(P(\\mathbf{c}^i \\vert y)\\)). (2) A “decoder” diffusion model generates the final image \\(\\mathbf{x}\\) conditioned on the image embedding \\(\\mathbf{c}^i\\) (and optionally the text \\(y\\)) (\\(P(\\mathbf{x} \\vert \\mathbf{c}^i, [y])\\)).\nImagen: Proposed by [Saharia et al., 2022]. Instead of CLIP, Imagen uses large, pre-trained frozen language models (like T5-XXL) as text encoders, finding that the scale of the text encoder was more critical than the scale of the diffusion U-Net. It introduced “Dynamic Thresholding” to improve image fidelity at high CFG scales by adaptively clipping predicted pixel values. It also proposed an “Efficient U-Net” architecture with modifications like shifting parameters to lower-resolution blocks and optimizing the order of up/downsampling operations.\nArchitectural Evolution (U-Net, DiT, ControlNet):\n\nU-Net: The classic architecture with downsampling/upsampling paths and skip connections remains a standard backbone for many diffusion models, especially in image domains.\nDiT (Diffusion Transformer): Proposed by [Peebles & Xie, 2023]. Adapts the Transformer architecture for diffusion, operating on latent patches (similar to LDM). It involves patchifying the latent representation, processing the sequence of patches through Transformer blocks, and incorporating conditioning (like timestep \\(t\\) and class \\(c\\)) via adaptive layer normalization (adaLN-Zero). DiT benefits from the known scalability of Transformers.\nControlNet: Proposed by [Zhang et al., 2023]. A technique for adding fine-grained spatial control (e.g., based on edge maps, human poses, depth maps) to large, pre-trained text-to-image diffusion models without expensive retraining. It works by creating a trainable copy of the model’s weights and connecting it to the original frozen weights via special “zero convolution” layers. These zero-initialized layers allow stable training of the control mechanism while preserving the original model’s capabilities. The output combines the original block’s output with the controlled copy’s output: \\(\\mathbf{y}_c = \\mathcal{F}_\\theta(\\mathbf{x}) + \\mathcal{Z}_{\\theta_{z2}}(\\mathcal{F}_{\\theta_c}(\\mathbf{x} + \\mathcal{Z}_{\\theta_{z1}}(\\mathbf{c})))\\)."
  },
  {
    "objectID": "posts/difusion-basics/index.html#summary",
    "href": "posts/difusion-basics/index.html#summary",
    "title": "Introduction to Diffusion Models: From Core Concepts to Cutting-Edge Applications",
    "section": "Summary",
    "text": "Summary\nDiffusion models represent a powerful and flexible class of generative models based on systematically destroying data structure with noise and then learning to reverse the process.\n\nAdvantages: They achieve state-of-the-art results in generating high-quality, diverse samples, particularly for images. They benefit from theoretical tractability and relatively stable training compared to alternatives like GANs.\nDisadvantages: Historically, their main drawback was slow sampling speed, requiring many sequential denoising steps. However, significant advancements (DDIM, LDM, distillation, consistency models) have drastically improved sampling efficiency, making them much more practical.\n\nFueled by innovations like Classifier-Free Guidance, Latent Diffusion, Transformer-based architectures, and control mechanisms like ControlNet, diffusion models are at the forefront of generative AI, enabling cutting-edge applications in text-to-image synthesis, image editing, video generation, and beyond."
  },
  {
    "objectID": "posts/difusion-basics/index.html#references",
    "href": "posts/difusion-basics/index.html#references",
    "title": "Introduction to Diffusion Models: From Core Concepts to Cutting-Edge Applications",
    "section": "References",
    "text": "References\n\nWeng, Lilian. (Jul 2021). What are diffusion models? Lil’Log. https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\nHo, Jonathan, Ajay Jain, and Pieter Abbeel. “Denoising diffusion probabilistic models.” NeurIPS 2020. (DDPM)\nSong, Jiaming, Chenlin Meng, and Stefano Ermon. “Denoising diffusion implicit models.” ICLR 2021. (DDIM)\nRombach, Robin, et al. “High-resolution image synthesis with latent diffusion models.” CVPR 2022. (Latent Diffusion / Foundation for Stable Diffusion)\nNichol, Alex, and Prafulla Dhariwal. “Improved denoising diffusion probabilistic models.” ICML 2021.\nDhariwal, Prafulla, and Alex Nichol. “Diffusion models beat gans on image synthesis.” NeurIPS 2021.\nHo, Jonathan, and Tim Salimans. “Classifier-free diffusion guidance.” NeurIPS 2021 Workshop.\nSalimans, Tim, and Jonathan Ho. “Progressive distillation for fast sampling of diffusion models.” ICLR 2022.\nSong, Yang, et al. “Consistency models.” ICML 2023.\nHo, Jonathan, et al. “Cascaded diffusion models for high fidelity image generation.” JMLR 2022.\nRamesh, Aditya, et al. “Hierarchical text-conditional image generation with clip latents.” arXiv 2022. (unCLIP / DALL-E 2)\nSaharia, Chitwan, et al. “Photorealistic text-to-image diffusion models with deep language understanding.” NeurIPS 2022. (Imagen)\nPeebles, William, and Saining Xie. “Scalable diffusion models with transformers.” ICCV 2023. (DiT)\nZhang, Lvmin, and Maneesh Agrawala. “Adding conditional control to text-to-image diffusion models.” ICCV 2023. (ControlNet)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Junichiro Iwasawa",
    "section": "",
    "text": "Personal webpage: jiwasawa.github.io/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Introduction to Diffusion Models: From Core Concepts to Cutting-Edge Applications\n\n\n\n\n\n\nMachine Learning\n\n\nDiffusion models\n\n\n\n\n\n\n\n\n\nApr 17, 2025\n\n\nJunichiro Iwasawa\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Self-Attention\n\n\n\n\n\n\nMachine Learning\n\n\nTransformer\n\n\nPython\n\n\nLLM\n\n\n\n\n\n\n\n\n\nApr 11, 2025\n\n\nJunichiro Iwasawa\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/transformer-attention/index.html",
    "href": "posts/transformer-attention/index.html",
    "title": "Understanding Self-Attention",
    "section": "",
    "text": "Lately, Large Language Models (LLMs) like ChatGPT and GPT-4 have taken the world by storm. These models demonstrate remarkable abilities, from generating code and drafting emails to answering complex questions and even writing creative prose. At the heart of many of these systems lies the Transformer architecture, introduced in the groundbreaking 2017 paper, “Attention is All You Need”.\nBut what exactly is this “Attention” mechanism, and how does it empower models like GPT to understand context and generate coherent text?\nAndrej Karpathy’s excellent video, “Let’s build GPT: from scratch, in code, spelled out.”, demystifies the Transformer by building a small version from the ground up, which he calls nanogpt. Let’s follow his lead and unravel the workings of self-attention, the core engine of the Transformer."
  },
  {
    "objectID": "posts/transformer-attention/index.html#getting-started-the-basics-of-language-modeling",
    "href": "posts/transformer-attention/index.html#getting-started-the-basics-of-language-modeling",
    "title": "Understanding Self-Attention",
    "section": "Getting Started: The Basics of Language Modeling",
    "text": "Getting Started: The Basics of Language Modeling\nBefore diving into attention, let’s grasp the fundamental task: language modeling. The goal of language modeling is to predict the next word (or character, or token) in a sequence, given the preceding sequence (the context).\nKarpathy starts with the “Tiny Shakespeare” dataset – a single text file containing concatenated works of Shakespeare.\n# First, let's prepare our training dataset. We'll download the Tiny Shakespeare dataset.\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n# Let's read it to see what's inside.\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# Let's list all the unique characters that occur in this text.\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\n# !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nprint(vocab_size)\n# 65\n\n# Create a mapping from characters to integers.\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\nprint(encode(\"hii there\"))\n# [46, 47, 47, 1, 58, 46, 43, 56, 43]\nprint(decode(encode(\"hii there\")))\n# hii there\n# Now let's encode the entire text dataset and store it into a torch.Tensor.\nimport torch # We use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\n# torch.Size([1115394]) torch.int64\nprint(data[:1000]) # The first 1,000 characters we looked at earlier will look like this to the GPT\n# tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44, ...\nIn this example, the text is tokenized at the character level, mapping each character to a number. The model’s job is to predict the next number in the sequence, given a sequence of numbers.\nKarpathy first implements the simplest possible language model: the Bigram Model.\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        # Later in the video, this changes to vocab_size x n_embd\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        # In the Bigram model, logits are looked up directly\n        logits = self.token_embedding_table(idx) # (B,T,C) where initially C=vocab_size\n\n        if targets is None:\n            loss = None\n        else:\n            # reshape for cross_entropy\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nm = BigramLanguageModel(vocab_size)\n# Assuming xb, yb are batches from get_batch function (not shown here, but in Karpathy's code)\n# logits, loss = m(xb, yb) \n# print(logits.shape) # Example shape if B=4, T=8 -&gt; (32, 65) -&gt; after view (B*T, C)\n# print(loss) # Example loss tensor\n\n# Generate text (assuming 'idx' starts as torch.zeros((1, 1), dtype=torch.long))\n# print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n# Example output (untrained): SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGpwnYWmnxKWWev-tDqXErVKLgJ\nLet’s train this simple model.\n# Create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nbatch_size = 32 # How many independent sequences will we process in parallel?\n# Assuming get_batch function exists as in Karpathy's code\n# def get_batch(split): ... return xb, yb\n\nfor steps in range(10000): # increase number of steps for good results...\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# print(loss.item()) # Example loss after some training\n# print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n# Example output (after some training): oTo.JUZ!!zqe!\n# xBP qbs$Gy'AcOmrLwwt ... (still mostly nonsense)\nThis model uses an embedding table where the index of the input character directly looks up a probability distribution (logits) for the next character. It’s simple, but has a critical flaw: it completely ignores context. The prediction after ‘t’ in “hat” is the same as after ‘t’ in “bat”. The tokens aren’t “talking” to each other."
  },
  {
    "objectID": "posts/transformer-attention/index.html#the-need-for-communication-aggregating-past-information",
    "href": "posts/transformer-attention/index.html#the-need-for-communication-aggregating-past-information",
    "title": "Understanding Self-Attention",
    "section": "The Need for Communication: Aggregating Past Information",
    "text": "The Need for Communication: Aggregating Past Information\nTo make better predictions, tokens need information from the tokens that came before them in the sequence. How can tokens communicate?\nKarpathy introduces a “mathematical trick” using matrix multiplication. The simplest way for a token to get context is to average the information from all preceding tokens, including itself.\nSuppose our input x has shape (B, T, C) (Batch, Time (sequence length), Channels (embedding dimension)). We want to compute xbow (a bag-of-words representation) such that xbow[b, t] contains the average of x[b, 0] through x[b, t].\nA simple loop is inefficient:\n# We want xbow[b,t] = mean_{i&lt;=t} x[b,i]\n# (assuming x is defined with shape B, T, C)\nB,T,C = 4,8,32 # example dimensions\nx = torch.randn(B,T,C)\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t+1, C)\n        xbow[b,t] = torch.mean(xprev, 0)\nA much more efficient way uses matrix multiplication with a lower-triangular matrix:\n# version 2: using matrix multiply for a weighted aggregation\nT = 8 # Example sequence length\n# toy example illustrating how matrix multiplication can be used for weighted aggregation.\nwei = torch.tril(torch.ones(T, T)) # Lower-triangular matrix of ones\nwei = wei / wei.sum(1, keepdim=True) # Normalize rows to sum to 1 -&gt; averaging\n# Example x with B=4, T=8, C=32\nx = torch.randn(4, T, 32)\nxbow2 = wei @ x # (T, T) @ (B, T, C) ----&gt; (B, T, C) due to broadcasting\ntorch.allclose(xbow, xbow2) # True\nHere, wei (weights) is a (T, T) matrix. Row t of wei has non-zero values (in this case, 1/(t+1)) only in columns 0 through t. Multiplying this with x (shape (B, T, C)), PyTorch broadcasts wei across the batch dimension. The resulting xbow2[b, t] becomes the weighted sum (average, in this case) of x[b, 0] through x[b, t].\nThis matrix multiplication efficiently performs the aggregation. We can also achieve this using softmax:\n# version 3: use Softmax\nT = 8\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf')) # Fill upper triangle with -inf\nwei = F.softmax(wei, dim=-1) # Softmax makes rows sum to 1, recovering averaging weights\nxbow3 = wei @ x\n# torch.allclose(xbow, xbow3) should be True\nWhy use softmax here? It introduces the crucial idea that the weights (wei) don’t have to be a fixed average; they can be learned or data-dependent. This is exactly what self-attention does."
  },
  {
    "objectID": "posts/transformer-attention/index.html#introducing-positional-information-position-encoding",
    "href": "posts/transformer-attention/index.html#introducing-positional-information-position-encoding",
    "title": "Understanding Self-Attention",
    "section": "Introducing Positional Information: Position Encoding",
    "text": "Introducing Positional Information: Position Encoding\nBefore diving into the self-attention mechanism itself, there’s another crucial element: information about the token’s position in the sequence.\nThe basic self-attention calculation (weighted aggregation using Query, Key, and Value) doesn’t inherently consider where tokens are located. If you shuffled the words in a sentence, the attention scores between any two given words (based on their vectors alone) wouldn’t change. This is problematic, as word order is fundamental to meaning. “The cat sat on the mat” means something very different from “The mat sat on the cat.”\nTo solve this, Transformers add a Position Encoding vector to the token’s own embedding vector (Token Embedding). This combined vector represents both the token’s meaning and its position.\nIn Karpathy’s nanogpt, learnable position encodings are used. Specifically, an embedding table (position_embedding_table) stores position vectors for up to the maximum sequence length (block_size). For a sequence of length T, integers from 0 to T-1 are used as indices to retrieve the corresponding position vectors from this table.\n# Excerpt from the forward method in BigramLanguageModel (or later GPTLanguageModel)\n# Assuming idx is the input tensor of token indices (B, T)\n# Assuming self.token_embedding_table and self.position_embedding_table are defined\n# Assuming n_embd is the embedding dimension C\n# Assuming block_size is the maximum context length\n# Assuming device is set ('cuda' or 'cpu')\n\nB, T = idx.shape\n\n# idx and targets are both (B,T) tensor of integers\ntok_emb = self.token_embedding_table(idx) # (B,T,C) - Token embeddings\n# torch.arange(T, device=device) generates integer sequence 0, 1, ..., T-1\npos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C) - Position embeddings\nx = tok_emb + pos_emb # (B,T,C) - Add token and position embeddings\n# x = self.blocks(x) # ... this x becomes the input to the Transformer blocks ...\nThis creates the vector x, containing both the token’s identity (tok_emb) and its position (pos_emb). This x is the actual input passed into the subsequent Transformer blocks (Self-Attention and FeedForward layers), allowing the model to consider both meaning and order."
  },
  {
    "objectID": "posts/transformer-attention/index.html#self-attention-data-dependent-information-aggregation",
    "href": "posts/transformer-attention/index.html#self-attention-data-dependent-information-aggregation",
    "title": "Understanding Self-Attention",
    "section": "Self-Attention: Data-Dependent Information Aggregation",
    "text": "Self-Attention: Data-Dependent Information Aggregation\nSimple averaging treats all past tokens equally. But often, some past tokens are much more relevant than others. When predicting the word after “The cat sat on the…”, the word “cat” is likely more important than “The”.\nSelf-attention allows tokens to query other tokens and assign attention scores based on relevance. Each token produces three vectors:\n\nQuery (Q): What am I looking for?\nKey (K): What information do I possess?\nValue (V): If attention is paid to me, what information will I provide?\n\nThe attention score (or affinity) between token i and token j is calculated by taking the dot product of token i’s Query vector (q_i) and token j’s Key vector (k_j):\naffinity(i, j) = q_i ⋅ k_j\nA high dot product means the Query matches the Key well, indicating token j is relevant to token i.\nHere’s how a single “Head” of attention is implemented:\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels (embedding dimension)\nx = torch.randn(B,T,C) # Input token embeddings + position encodings\n\n# Let's see a single Head perform self-attention\nhead_size = 16 # Dimension of K, Q, V vectors for this head\n# Linear layers to project input 'x' into K, Q, V\nkey   = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\n\nk = key(x)   # (B, T, head_size)\nq = query(x) # (B, T, head_size)\n\n# compute attention scores (\"affinities\")\n# (B, T, head_size) @ (B, head_size, T) ---&gt; (B, T, T)\nwei =  q @ k.transpose(-2, -1) # wei means \"weights\" or scores\n\n# --- Scaling Step (discussed below) ---\n# Scale the affinities\nwei = wei * (head_size**-0.5) \n\n# --- Masking for Decoder ---\n# Assume T is the sequence length (e.g., 8 here)\n# Assume x.device holds the correct device ('cuda' or 'cpu')\ntril = torch.tril(torch.ones(T, T, device=x.device)) \n# Mask out future tokens\nwei = wei.masked_fill(tril == 0, float('-inf')) \n\n# --- Normalize Scores to Probabilities ---\nwei = F.softmax(wei, dim=-1) # (B, T, T)\n\n# --- Perform the weighted aggregation of Values ---\nv = value(x) # (B, T, head_size)\n# (B, T, T) @ (B, T, head_size) ---&gt; (B, T, head_size)\nout = wei @ v\n\n# out.shape is (B, T, head_size)\nLet’s break down the key steps:\n\nProjection: The input x (containing token + position info) is projected into K, Q, and V spaces using linear layers.\nAffinity Calculation: q @ k.transpose(...) computes the dot product between every pair of Query and Key vectors within each sequence in the batch. This yields wei, the raw attention scores (shape B, T, T).\nScaling: The scores wei are scaled down by the square root of head_size. This is crucial for stabilizing training, especially during initialization. Without scaling, the variance of the dot products grows with head_size, potentially pushing the inputs to softmax into regions with tiny gradients, hindering learning. This is the “Scaled” part of “Scaled Dot-Product Attention”.\nMasking (Decoder-Specific): In autoregressive language modeling like GPT, a token at position t should only attend to tokens up to position t. This is achieved by setting the attention scores corresponding to future positions (j &gt; t) to negative infinity using masked_fill with a lower-triangular matrix (tril). Softmax then assigns zero probability to these future tokens. (Note: Encoder blocks, like in BERT, do not use this causal mask).\nSoftmax: Softmax is applied row-wise to the masked scores. This converts the scores into probabilities that sum to 1 for each token t, representing the attention distribution over preceding tokens 0 to t.\nValue Aggregation: The final output out for each token t is a weighted sum of the Value vectors (v) of all tokens, weighted by the attention probabilities in wei. out = wei @ v.\n\nThe output out (shape B, T, head_size) contains, for each token, aggregated information from other relevant tokens in the sequence, based on the learned K, Q, V projections."
  },
  {
    "objectID": "posts/transformer-attention/index.html#multi-head-attention-multiple-perspectives",
    "href": "posts/transformer-attention/index.html#multi-head-attention-multiple-perspectives",
    "title": "Understanding Self-Attention",
    "section": "Multi-Head Attention: Multiple Perspectives",
    "text": "Multi-Head Attention: Multiple Perspectives\nA single attention head might learn to focus on a specific type of relationship (e.g., noun-verb agreement). To capture diverse relationships, Transformers use Multi-Head Attention.\n# Assuming n_embd, block_size, dropout are defined hyperparameters\n# n_embd = 384 # Example embedding dimension\n# block_size = 256 # Example max context length\n# dropout = 0.2 # Example dropout rate\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        # tril is registered as a buffer (not a parameter)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout) # Add dropout\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,head_size)\n        q = self.query(x) # (B,T,head_size)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # scale by head_size**-0.5\n        # Apply mask dynamically based on T\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Use only up to T x T part of tril\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei) # Apply dropout to attention weights\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,head_size)\n        out = wei @ v # (B,T,head_size)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        # Create multiple Head instances\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        # Projection layer after concatenation\n        self.proj = nn.Linear(num_heads * head_size, n_embd) # n_embd = num_heads * head_size assumed\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # Run each head in parallel and concatenate results along the channel dimension\n        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, num_heads * head_size)\n        # Re-project the concatenated output back to the original n_embd dimension\n        out = self.dropout(self.proj(out)) # (B, T, n_embd)\n        return out\nThis simply runs multiple Head modules in parallel, potentially each with different learned K, Q, V projections. The outputs of each head (each B, T, head_size) are concatenated (B, T, num_heads * head_size) and then projected back to the original embedding dimension (B, T, n_embd) using another linear layer (self.proj). This allows the model to simultaneously attend to information from different representation subspaces."
  },
  {
    "objectID": "posts/transformer-attention/index.html#attention-flavors-self-cross-encoders-decoders",
    "href": "posts/transformer-attention/index.html#attention-flavors-self-cross-encoders-decoders",
    "title": "Understanding Self-Attention",
    "section": "Attention Flavors: Self, Cross, Encoders & Decoders",
    "text": "Attention Flavors: Self, Cross, Encoders & Decoders\nThe basic mechanism we’ve discussed so far is often called Self-Attention because the Query (Q), Key (K), and Value (V) vectors are all derived from the same input sequence (x), allowing tokens within that sequence to attend to each other. However, there are important variations in how self-attention is used and in the broader attention mechanism.\nFirstly, how self-attention is used differs between Encoder and Decoder blocks, primarily due to masking.\nSelf-attention in a Decoder block employs causal masking (the triangular mask) to prevent tokens from attending to future positions. This is essential for autoregressive models like GPT or the decoder part of a machine translation model, where generation must rely only on past information. Karpathy’s nanogpt is precisely a model composed only of these Decoder blocks.\nConversely, self-attention in an Encoder block does not use causal masking. All tokens in the sequence can freely attend to all other tokens (past and future). This is used in models like BERT, which aim to understand the full context of an input text, or in the encoder part of a machine translation model (which encodes the entire source sentence). It’s suited for capturing bidirectional context.\nSecondly, another crucial form of attention is Cross-Attention. Unlike self-attention (masked or unmasked), the sources for Query, Key, and Value differ. In cross-attention, the Query (Q) typically comes from one source (e.g., the decoder’s state), while the Key (K) and Value (V) come from another source (e.g., the final output of the encoder).\nCross-attention primarily serves to connect the Encoder and Decoder in an Encoder-Decoder architecture. It allows the decoder, as it generates output tokens, to continually refer back to the entire encoded input information via the K and V vectors from the encoder. This enables tasks like machine translation, where the model needs to consider the meaning of the source sentence while generating the target language.\nSince nanogpt is a decoder-only model, it doesn’t have an encoder to process an external input sequence. Therefore, it doesn’t need Encoder blocks or Cross-Attention; it consists solely of Self-Attention with causal masking (Decoder blocks)."
  },
  {
    "objectID": "posts/transformer-attention/index.html#the-transformer-block-communication-and-computation",
    "href": "posts/transformer-attention/index.html#the-transformer-block-communication-and-computation",
    "title": "Understanding Self-Attention",
    "section": "The Transformer Block: Communication and Computation",
    "text": "The Transformer Block: Communication and Computation\nAttention provides the communication mechanism. But the model also needs computation to process the aggregated information. A standard Transformer block combines Multi-Head Self-Attention with a simple, position-wise FeedForward network.\nCrucially, Residual Connections and Layer Normalization are added around each sub-layer (Attention and FeedForward).\n\nResidual Connections: x = x + sublayer(norm(x)). The input x to the sub-layer is added to the output of the sub-layer. This significantly helps gradients flow during backpropagation in deep networks, stabilizing training and improving performance.\nLayer Normalization: Normalizes the features independently for each token across the channel dimension. Unlike Batch Normalization, it doesn’t rely on batch statistics, making it well-suited for sequence data. It also stabilizes training. Karpathy implements the common “pre-norm” formulation, where LayerNorm is applied before the sub-layer.\n\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd), # Inner layer is typically 4x larger\n            nn.ReLU(),                     # ReLU activation\n            nn.Linear(4 * n_embd, n_embd), # Project back to n_embd\n            nn.Dropout(dropout),            # Dropout for regularization\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size) # Communication\n        self.ffwd = FeedFoward(n_embd)                  # Computation\n        self.ln1 = nn.LayerNorm(n_embd)                 # LayerNorm before Attention\n        self.ln2 = nn.LayerNorm(n_embd)                 # LayerNorm before FeedForward\n\n    def forward(self, x):\n        # Pre-norm formulation with residual connections\n        # Apply LayerNorm -&gt; Self-Attention -&gt; Add residual\n        x = x + self.sa(self.ln1(x))\n        # Apply LayerNorm -&gt; FeedForward -&gt; Add residual\n        x = x + self.ffwd(self.ln2(x))\n        return x\nA full GPT model simply stacks multiple of these Block layers sequentially. After passing through all blocks, a final LayerNorm is applied, followed by a linear layer that projects the final token representations to the vocabulary size, yielding logits for predicting the next token."
  },
  {
    "objectID": "posts/transformer-attention/index.html#putting-it-all-together-the-final-gpt-model",
    "href": "posts/transformer-attention/index.html#putting-it-all-together-the-final-gpt-model",
    "title": "Understanding Self-Attention",
    "section": "Putting it all Together: The Final GPT Model",
    "text": "Putting it all Together: The Final GPT Model\nIntegrating the components discussed, we arrive at the final GPT-style language model, GPTLanguageModel. The code below represents the completed version from Karpathy’s video, incorporating the Block (which includes MultiHeadAttention and FeedForward) and other elements.\n# (Reiterating key hyperparameters)\n# hyperparameters\nbatch_size = 64 # how many independent sequences will we process in parallel?\nblock_size = 256 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384 # embedding dimension\nn_head = 6   # number of attention heads\nn_layer = 6  # number of Transformer blocks (layers)\ndropout = 0.2 # dropout rate\n# ------------\n\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        # Stack n_layer Transformer blocks\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size) # output layer (linear)\n\n        # Better weight initialization (important but not covered in detail in the video walk-through)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        # (Weight initialization details omitted for brevity, see Karpathy's code)\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C) Pass through Transformer blocks\n        x = self.ln_f(x) # (B,T,C) Apply final LayerNorm\n        logits = self.lm_head(x) # (B,T,vocab_size) Compute logits via LM head\n\n        if targets is None:\n            loss = None\n        else:\n            # Reshape for loss calculation\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens due to position embedding size limit\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond) # perform forward pass\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\n# Example Usage (assuming training loop and data loading are set up)\n# model = GPTLanguageModel()\n# m = model.to(device)\n# ... training loop using optimizer and get_batch ...\n# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n# print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\nIn this GPTLanguageModel class, the __init__ method defines the token and position embedding tables, stacks n_layer Blocks using nn.Sequential (the core Transformer), adds a final LayerNorm (ln_f), and the output linear layer (lm_head). It also includes the _init_weights method crucial for stable training.\nThe forward method implements the flow: add token and position embeddings, pass through the blocks, apply final normalization, and project to logits.\nThe generate method produces text autoregressively. The key line idx_cond = idx[:, -block_size:] highlights a constraint: because the position_embedding_table has a fixed size (block_size), the model can only condition on the most recent block_size tokens when making a prediction. Within this context window, it performs a forward pass, samples the next token based on the final timestep’s logits, and extends the sequence.\nThe complete code also involves hyperparameters (like batch_size, learning_rate), an AdamW optimizer, and a standard training loop with evaluation (using an estimate_loss function), all working together to train and run the GPT model."
  },
  {
    "objectID": "posts/transformer-attention/index.html#scaling-up-and-results",
    "href": "posts/transformer-attention/index.html#scaling-up-and-results",
    "title": "Understanding Self-Attention",
    "section": "Scaling Up and Results",
    "text": "Scaling Up and Results\nKarpathy trains this GPTLanguageModel (with n_layer=6, n_head=6, n_embd=384, dropout=0.2) on Tiny Shakespeare. The resulting model generates much more coherent (though still nonsensical) Shakespeare-like text, demonstrating the power of attention combined with sufficient model capacity.\n# Sample output from the trained GPTLanguageModel\nFlY BOLINGLO:\nThem thrumply towiter arts the\nmuscue rike begatt the sea it\nWhat satell in rowers that some than othis Marrity.\n\nLUCENTVO:\nBut userman these that, where can is not diesty rege;\nWhat and see to not. But's eyes. What?\nThis architecture—the decoder-only Transformer (using causal masking)—is fundamentally the same as that used in models like GPT-2 and GPT-3, just massively scaled up in terms of the number of parameters, layers, embedding sizes, and, crucially, the training data (vast amounts of internet text instead of just Shakespeare)."
  },
  {
    "objectID": "posts/transformer-attention/index.html#conclusion",
    "href": "posts/transformer-attention/index.html#conclusion",
    "title": "Understanding Self-Attention",
    "section": "Conclusion",
    "text": "Conclusion\nThe attention mechanism, specifically scaled dot-product self-attention, is the innovation that unlocked the power of Transformers. It allows tokens in a sequence to dynamically query each other, compute relevance scores (affinities) based on learned Query-Key interactions, and aggregate information from relevant tokens’ Value vectors in a weighted manner. Combined with Multi-Head Attention, Residual Connections, Layer Normalization, and position-wise FeedForward networks, it forms the Transformer block – the fundamental building block of the models revolutionizing AI today.\nBy building it up step-by-step, as Karpathy demonstrates, we see that while powerful, the core ideas are graspable and can be implemented with relatively concise code.\n\nThis post is based on Andrej Karpathy’s YouTube video “Let’s build GPT: from scratch, in code, spelled out.”. For the complete code and deeper insights, definitely check out the video and his nanogpt repository. Hopefully, this walkthrough helps clarify the magic behind Transformers and Attention!"
  }
]