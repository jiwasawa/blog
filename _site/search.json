[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Junichiro Iwasawa",
    "section": "",
    "text": "Personal webpage: jiwasawa.github.io/"
  },
  {
    "objectID": "posts/transformer-attention/index.html",
    "href": "posts/transformer-attention/index.html",
    "title": "Understanding Self-Attention",
    "section": "",
    "text": "Lately, Large Language Models (LLMs) like ChatGPT and GPT-4 have taken the world by storm. These models demonstrate remarkable abilities, from generating code and drafting emails to answering complex questions and even writing creative prose. At the heart of many of these systems lies the Transformer architecture, introduced in the groundbreaking 2017 paper, “Attention is All You Need”.\nBut what exactly is this “Attention” mechanism, and how does it empower models like GPT to understand context and generate coherent text?\nAndrej Karpathy’s excellent video, “Let’s build GPT: from scratch, in code, spelled out.”, demystifies the Transformer by building a small version from the ground up, which he calls nanogpt. Let’s follow his lead and unravel the workings of self-attention, the core engine of the Transformer."
  },
  {
    "objectID": "posts/transformer-attention/index.html#getting-started-the-basics-of-language-modeling",
    "href": "posts/transformer-attention/index.html#getting-started-the-basics-of-language-modeling",
    "title": "Understanding Self-Attention",
    "section": "Getting Started: The Basics of Language Modeling",
    "text": "Getting Started: The Basics of Language Modeling\nBefore diving into attention, let’s grasp the fundamental task: language modeling. The goal of language modeling is to predict the next word (or character, or token) in a sequence, given the preceding sequence (the context).\nKarpathy starts with the “Tiny Shakespeare” dataset – a single text file containing concatenated works of Shakespeare.\n# First, let's prepare our training dataset. We'll download the Tiny Shakespeare dataset.\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n# Let's read it to see what's inside.\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# Let's list all the unique characters that occur in this text.\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\n# !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nprint(vocab_size)\n# 65\n\n# Create a mapping from characters to integers.\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\nprint(encode(\"hii there\"))\n# [46, 47, 47, 1, 58, 46, 43, 56, 43]\nprint(decode(encode(\"hii there\")))\n# hii there\n# Now let's encode the entire text dataset and store it into a torch.Tensor.\nimport torch # We use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\n# torch.Size([1115394]) torch.int64\nprint(data[:1000]) # The first 1,000 characters we looked at earlier will look like this to the GPT\n# tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44, ...\nIn this example, the text is tokenized at the character level, mapping each character to a number. The model’s job is to predict the next number in the sequence, given a sequence of numbers.\nKarpathy first implements the simplest possible language model: the Bigram Model.\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        # Later in the video, this changes to vocab_size x n_embd\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        # In the Bigram model, logits are looked up directly\n        logits = self.token_embedding_table(idx) # (B,T,C) where initially C=vocab_size\n\n        if targets is None:\n            loss = None\n        else:\n            # reshape for cross_entropy\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nm = BigramLanguageModel(vocab_size)\n# Assuming xb, yb are batches from get_batch function (not shown here, but in Karpathy's code)\n# logits, loss = m(xb, yb) \n# print(logits.shape) # Example shape if B=4, T=8 -&gt; (32, 65) -&gt; after view (B*T, C)\n# print(loss) # Example loss tensor\n\n# Generate text (assuming 'idx' starts as torch.zeros((1, 1), dtype=torch.long))\n# print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n# Example output (untrained): SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGpwnYWmnxKWWev-tDqXErVKLgJ\nLet’s train this simple model.\n# Create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nbatch_size = 32 # How many independent sequences will we process in parallel?\n# Assuming get_batch function exists as in Karpathy's code\n# def get_batch(split): ... return xb, yb\n\nfor steps in range(10000): # increase number of steps for good results...\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# print(loss.item()) # Example loss after some training\n# print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n# Example output (after some training): oTo.JUZ!!zqe!\n# xBP qbs$Gy'AcOmrLwwt ... (still mostly nonsense)\nThis model uses an embedding table where the index of the input character directly looks up a probability distribution (logits) for the next character. It’s simple, but has a critical flaw: it completely ignores context. The prediction after ‘t’ in “hat” is the same as after ‘t’ in “bat”. The tokens aren’t “talking” to each other."
  },
  {
    "objectID": "posts/transformer-attention/index.html#the-need-for-communication-aggregating-past-information",
    "href": "posts/transformer-attention/index.html#the-need-for-communication-aggregating-past-information",
    "title": "Understanding Self-Attention",
    "section": "The Need for Communication: Aggregating Past Information",
    "text": "The Need for Communication: Aggregating Past Information\nTo make better predictions, tokens need information from the tokens that came before them in the sequence. How can tokens communicate?\nKarpathy introduces a “mathematical trick” using matrix multiplication. The simplest way for a token to get context is to average the information from all preceding tokens, including itself.\nSuppose our input x has shape (B, T, C) (Batch, Time (sequence length), Channels (embedding dimension)). We want to compute xbow (a bag-of-words representation) such that xbow[b, t] contains the average of x[b, 0] through x[b, t].\nA simple loop is inefficient:\n# We want xbow[b,t] = mean_{i&lt;=t} x[b,i]\n# (assuming x is defined with shape B, T, C)\nB,T,C = 4,8,32 # example dimensions\nx = torch.randn(B,T,C)\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t+1, C)\n        xbow[b,t] = torch.mean(xprev, 0)\nA much more efficient way uses matrix multiplication with a lower-triangular matrix:\n# version 2: using matrix multiply for a weighted aggregation\nT = 8 # Example sequence length\n# toy example illustrating how matrix multiplication can be used for weighted aggregation.\nwei = torch.tril(torch.ones(T, T)) # Lower-triangular matrix of ones\nwei = wei / wei.sum(1, keepdim=True) # Normalize rows to sum to 1 -&gt; averaging\n# Example x with B=4, T=8, C=32\nx = torch.randn(4, T, 32)\nxbow2 = wei @ x # (T, T) @ (B, T, C) ----&gt; (B, T, C) due to broadcasting\ntorch.allclose(xbow, xbow2) # True\nHere, wei (weights) is a (T, T) matrix. Row t of wei has non-zero values (in this case, 1/(t+1)) only in columns 0 through t. Multiplying this with x (shape (B, T, C)), PyTorch broadcasts wei across the batch dimension. The resulting xbow2[b, t] becomes the weighted sum (average, in this case) of x[b, 0] through x[b, t].\nThis matrix multiplication efficiently performs the aggregation. We can also achieve this using softmax:\n# version 3: use Softmax\nT = 8\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf')) # Fill upper triangle with -inf\nwei = F.softmax(wei, dim=-1) # Softmax makes rows sum to 1, recovering averaging weights\nxbow3 = wei @ x\n# torch.allclose(xbow, xbow3) should be True\nWhy use softmax here? It introduces the crucial idea that the weights (wei) don’t have to be a fixed average; they can be learned or data-dependent. This is exactly what self-attention does."
  },
  {
    "objectID": "posts/transformer-attention/index.html#introducing-positional-information-position-encoding",
    "href": "posts/transformer-attention/index.html#introducing-positional-information-position-encoding",
    "title": "Understanding Self-Attention",
    "section": "Introducing Positional Information: Position Encoding",
    "text": "Introducing Positional Information: Position Encoding\nBefore diving into the self-attention mechanism itself, there’s another crucial element: information about the token’s position in the sequence.\nThe basic self-attention calculation (weighted aggregation using Query, Key, and Value) doesn’t inherently consider where tokens are located. If you shuffled the words in a sentence, the attention scores between any two given words (based on their vectors alone) wouldn’t change. This is problematic, as word order is fundamental to meaning. “The cat sat on the mat” means something very different from “The mat sat on the cat.”\nTo solve this, Transformers add a Position Encoding vector to the token’s own embedding vector (Token Embedding). This combined vector represents both the token’s meaning and its position.\nIn Karpathy’s nanogpt, learnable position encodings are used. Specifically, an embedding table (position_embedding_table) stores position vectors for up to the maximum sequence length (block_size). For a sequence of length T, integers from 0 to T-1 are used as indices to retrieve the corresponding position vectors from this table.\n# Excerpt from the forward method in BigramLanguageModel (or later GPTLanguageModel)\n# Assuming idx is the input tensor of token indices (B, T)\n# Assuming self.token_embedding_table and self.position_embedding_table are defined\n# Assuming n_embd is the embedding dimension C\n# Assuming block_size is the maximum context length\n# Assuming device is set ('cuda' or 'cpu')\n\nB, T = idx.shape\n\n# idx and targets are both (B,T) tensor of integers\ntok_emb = self.token_embedding_table(idx) # (B,T,C) - Token embeddings\n# torch.arange(T, device=device) generates integer sequence 0, 1, ..., T-1\npos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C) - Position embeddings\nx = tok_emb + pos_emb # (B,T,C) - Add token and position embeddings\n# x = self.blocks(x) # ... this x becomes the input to the Transformer blocks ...\nThis creates the vector x, containing both the token’s identity (tok_emb) and its position (pos_emb). This x is the actual input passed into the subsequent Transformer blocks (Self-Attention and FeedForward layers), allowing the model to consider both meaning and order."
  },
  {
    "objectID": "posts/transformer-attention/index.html#self-attention-data-dependent-information-aggregation",
    "href": "posts/transformer-attention/index.html#self-attention-data-dependent-information-aggregation",
    "title": "Understanding Self-Attention",
    "section": "Self-Attention: Data-Dependent Information Aggregation",
    "text": "Self-Attention: Data-Dependent Information Aggregation\nSimple averaging treats all past tokens equally. But often, some past tokens are much more relevant than others. When predicting the word after “The cat sat on the…”, the word “cat” is likely more important than “The”.\nSelf-attention allows tokens to query other tokens and assign attention scores based on relevance. Each token produces three vectors:\n\nQuery (Q): What am I looking for?\nKey (K): What information do I possess?\nValue (V): If attention is paid to me, what information will I provide?\n\nThe attention score (or affinity) between token i and token j is calculated by taking the dot product of token i’s Query vector (q_i) and token j’s Key vector (k_j):\naffinity(i, j) = q_i ⋅ k_j\nA high dot product means the Query matches the Key well, indicating token j is relevant to token i.\nHere’s how a single “Head” of attention is implemented:\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels (embedding dimension)\nx = torch.randn(B,T,C) # Input token embeddings + position encodings\n\n# Let's see a single Head perform self-attention\nhead_size = 16 # Dimension of K, Q, V vectors for this head\n# Linear layers to project input 'x' into K, Q, V\nkey   = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\n\nk = key(x)   # (B, T, head_size)\nq = query(x) # (B, T, head_size)\n\n# compute attention scores (\"affinities\")\n# (B, T, head_size) @ (B, head_size, T) ---&gt; (B, T, T)\nwei =  q @ k.transpose(-2, -1) # wei means \"weights\" or scores\n\n# --- Scaling Step (discussed below) ---\n# Scale the affinities\nwei = wei * (head_size**-0.5) \n\n# --- Masking for Decoder ---\n# Assume T is the sequence length (e.g., 8 here)\n# Assume x.device holds the correct device ('cuda' or 'cpu')\ntril = torch.tril(torch.ones(T, T, device=x.device)) \n# Mask out future tokens\nwei = wei.masked_fill(tril == 0, float('-inf')) \n\n# --- Normalize Scores to Probabilities ---\nwei = F.softmax(wei, dim=-1) # (B, T, T)\n\n# --- Perform the weighted aggregation of Values ---\nv = value(x) # (B, T, head_size)\n# (B, T, T) @ (B, T, head_size) ---&gt; (B, T, head_size)\nout = wei @ v\n\n# out.shape is (B, T, head_size)\nLet’s break down the key steps:\n\nProjection: The input x (containing token + position info) is projected into K, Q, and V spaces using linear layers.\nAffinity Calculation: q @ k.transpose(...) computes the dot product between every pair of Query and Key vectors within each sequence in the batch. This yields wei, the raw attention scores (shape B, T, T).\nScaling: The scores wei are scaled down by the square root of head_size. This is crucial for stabilizing training, especially during initialization. Without scaling, the variance of the dot products grows with head_size, potentially pushing the inputs to softmax into regions with tiny gradients, hindering learning. This is the “Scaled” part of “Scaled Dot-Product Attention”.\nMasking (Decoder-Specific): In autoregressive language modeling like GPT, a token at position t should only attend to tokens up to position t. This is achieved by setting the attention scores corresponding to future positions (j &gt; t) to negative infinity using masked_fill with a lower-triangular matrix (tril). Softmax then assigns zero probability to these future tokens. (Note: Encoder blocks, like in BERT, do not use this causal mask).\nSoftmax: Softmax is applied row-wise to the masked scores. This converts the scores into probabilities that sum to 1 for each token t, representing the attention distribution over preceding tokens 0 to t.\nValue Aggregation: The final output out for each token t is a weighted sum of the Value vectors (v) of all tokens, weighted by the attention probabilities in wei. out = wei @ v.\n\nThe output out (shape B, T, head_size) contains, for each token, aggregated information from other relevant tokens in the sequence, based on the learned K, Q, V projections."
  },
  {
    "objectID": "posts/transformer-attention/index.html#multi-head-attention-multiple-perspectives",
    "href": "posts/transformer-attention/index.html#multi-head-attention-multiple-perspectives",
    "title": "Understanding Self-Attention",
    "section": "Multi-Head Attention: Multiple Perspectives",
    "text": "Multi-Head Attention: Multiple Perspectives\nA single attention head might learn to focus on a specific type of relationship (e.g., noun-verb agreement). To capture diverse relationships, Transformers use Multi-Head Attention.\n# Assuming n_embd, block_size, dropout are defined hyperparameters\n# n_embd = 384 # Example embedding dimension\n# block_size = 256 # Example max context length\n# dropout = 0.2 # Example dropout rate\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        # tril is registered as a buffer (not a parameter)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout) # Add dropout\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,head_size)\n        q = self.query(x) # (B,T,head_size)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # scale by head_size**-0.5\n        # Apply mask dynamically based on T\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Use only up to T x T part of tril\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei) # Apply dropout to attention weights\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,head_size)\n        out = wei @ v # (B,T,head_size)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        # Create multiple Head instances\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        # Projection layer after concatenation\n        self.proj = nn.Linear(num_heads * head_size, n_embd) # n_embd = num_heads * head_size assumed\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # Run each head in parallel and concatenate results along the channel dimension\n        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, num_heads * head_size)\n        # Re-project the concatenated output back to the original n_embd dimension\n        out = self.dropout(self.proj(out)) # (B, T, n_embd)\n        return out\nThis simply runs multiple Head modules in parallel, potentially each with different learned K, Q, V projections. The outputs of each head (each B, T, head_size) are concatenated (B, T, num_heads * head_size) and then projected back to the original embedding dimension (B, T, n_embd) using another linear layer (self.proj). This allows the model to simultaneously attend to information from different representation subspaces."
  },
  {
    "objectID": "posts/transformer-attention/index.html#attention-flavors-self-cross-encoders-decoders",
    "href": "posts/transformer-attention/index.html#attention-flavors-self-cross-encoders-decoders",
    "title": "Understanding Self-Attention",
    "section": "Attention Flavors: Self, Cross, Encoders & Decoders",
    "text": "Attention Flavors: Self, Cross, Encoders & Decoders\nThe basic mechanism we’ve discussed so far is often called Self-Attention because the Query (Q), Key (K), and Value (V) vectors are all derived from the same input sequence (x), allowing tokens within that sequence to attend to each other. However, there are important variations in how self-attention is used and in the broader attention mechanism.\nFirstly, how self-attention is used differs between Encoder and Decoder blocks, primarily due to masking.\nSelf-attention in a Decoder block employs causal masking (the triangular mask) to prevent tokens from attending to future positions. This is essential for autoregressive models like GPT or the decoder part of a machine translation model, where generation must rely only on past information. Karpathy’s nanogpt is precisely a model composed only of these Decoder blocks.\nConversely, self-attention in an Encoder block does not use causal masking. All tokens in the sequence can freely attend to all other tokens (past and future). This is used in models like BERT, which aim to understand the full context of an input text, or in the encoder part of a machine translation model (which encodes the entire source sentence). It’s suited for capturing bidirectional context.\nSecondly, another crucial form of attention is Cross-Attention. Unlike self-attention (masked or unmasked), the sources for Query, Key, and Value differ. In cross-attention, the Query (Q) typically comes from one source (e.g., the decoder’s state), while the Key (K) and Value (V) come from another source (e.g., the final output of the encoder).\nCross-attention primarily serves to connect the Encoder and Decoder in an Encoder-Decoder architecture. It allows the decoder, as it generates output tokens, to continually refer back to the entire encoded input information via the K and V vectors from the encoder. This enables tasks like machine translation, where the model needs to consider the meaning of the source sentence while generating the target language.\nSince nanogpt is a decoder-only model, it doesn’t have an encoder to process an external input sequence. Therefore, it doesn’t need Encoder blocks or Cross-Attention; it consists solely of Self-Attention with causal masking (Decoder blocks)."
  },
  {
    "objectID": "posts/transformer-attention/index.html#the-transformer-block-communication-and-computation",
    "href": "posts/transformer-attention/index.html#the-transformer-block-communication-and-computation",
    "title": "Understanding Self-Attention",
    "section": "The Transformer Block: Communication and Computation",
    "text": "The Transformer Block: Communication and Computation\nAttention provides the communication mechanism. But the model also needs computation to process the aggregated information. A standard Transformer block combines Multi-Head Self-Attention with a simple, position-wise FeedForward network.\nCrucially, Residual Connections and Layer Normalization are added around each sub-layer (Attention and FeedForward).\n\nResidual Connections: x = x + sublayer(norm(x)). The input x to the sub-layer is added to the output of the sub-layer. This significantly helps gradients flow during backpropagation in deep networks, stabilizing training and improving performance.\nLayer Normalization: Normalizes the features independently for each token across the channel dimension. Unlike Batch Normalization, it doesn’t rely on batch statistics, making it well-suited for sequence data. It also stabilizes training. Karpathy implements the common “pre-norm” formulation, where LayerNorm is applied before the sub-layer.\n\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd), # Inner layer is typically 4x larger\n            nn.ReLU(),                     # ReLU activation\n            nn.Linear(4 * n_embd, n_embd), # Project back to n_embd\n            nn.Dropout(dropout),            # Dropout for regularization\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size) # Communication\n        self.ffwd = FeedFoward(n_embd)                  # Computation\n        self.ln1 = nn.LayerNorm(n_embd)                 # LayerNorm before Attention\n        self.ln2 = nn.LayerNorm(n_embd)                 # LayerNorm before FeedForward\n\n    def forward(self, x):\n        # Pre-norm formulation with residual connections\n        # Apply LayerNorm -&gt; Self-Attention -&gt; Add residual\n        x = x + self.sa(self.ln1(x))\n        # Apply LayerNorm -&gt; FeedForward -&gt; Add residual\n        x = x + self.ffwd(self.ln2(x))\n        return x\nA full GPT model simply stacks multiple of these Block layers sequentially. After passing through all blocks, a final LayerNorm is applied, followed by a linear layer that projects the final token representations to the vocabulary size, yielding logits for predicting the next token."
  },
  {
    "objectID": "posts/transformer-attention/index.html#putting-it-all-together-the-final-gpt-model",
    "href": "posts/transformer-attention/index.html#putting-it-all-together-the-final-gpt-model",
    "title": "Understanding Self-Attention",
    "section": "Putting it all Together: The Final GPT Model",
    "text": "Putting it all Together: The Final GPT Model\nIntegrating the components discussed, we arrive at the final GPT-style language model, GPTLanguageModel. The code below represents the completed version from Karpathy’s video, incorporating the Block (which includes MultiHeadAttention and FeedForward) and other elements.\n# (Reiterating key hyperparameters)\n# hyperparameters\nbatch_size = 64 # how many independent sequences will we process in parallel?\nblock_size = 256 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384 # embedding dimension\nn_head = 6   # number of attention heads\nn_layer = 6  # number of Transformer blocks (layers)\ndropout = 0.2 # dropout rate\n# ------------\n\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        # Stack n_layer Transformer blocks\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size) # output layer (linear)\n\n        # Better weight initialization (important but not covered in detail in the video walk-through)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        # (Weight initialization details omitted for brevity, see Karpathy's code)\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C) Pass through Transformer blocks\n        x = self.ln_f(x) # (B,T,C) Apply final LayerNorm\n        logits = self.lm_head(x) # (B,T,vocab_size) Compute logits via LM head\n\n        if targets is None:\n            loss = None\n        else:\n            # Reshape for loss calculation\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens due to position embedding size limit\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond) # perform forward pass\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\n# Example Usage (assuming training loop and data loading are set up)\n# model = GPTLanguageModel()\n# m = model.to(device)\n# ... training loop using optimizer and get_batch ...\n# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n# print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\nIn this GPTLanguageModel class, the __init__ method defines the token and position embedding tables, stacks n_layer Blocks using nn.Sequential (the core Transformer), adds a final LayerNorm (ln_f), and the output linear layer (lm_head). It also includes the _init_weights method crucial for stable training.\nThe forward method implements the flow: add token and position embeddings, pass through the blocks, apply final normalization, and project to logits.\nThe generate method produces text autoregressively. The key line idx_cond = idx[:, -block_size:] highlights a constraint: because the position_embedding_table has a fixed size (block_size), the model can only condition on the most recent block_size tokens when making a prediction. Within this context window, it performs a forward pass, samples the next token based on the final timestep’s logits, and extends the sequence.\nThe complete code also involves hyperparameters (like batch_size, learning_rate), an AdamW optimizer, and a standard training loop with evaluation (using an estimate_loss function), all working together to train and run the GPT model."
  },
  {
    "objectID": "posts/transformer-attention/index.html#scaling-up-and-results",
    "href": "posts/transformer-attention/index.html#scaling-up-and-results",
    "title": "Understanding Self-Attention",
    "section": "Scaling Up and Results",
    "text": "Scaling Up and Results\nKarpathy trains this GPTLanguageModel (with n_layer=6, n_head=6, n_embd=384, dropout=0.2) on Tiny Shakespeare. The resulting model generates much more coherent (though still nonsensical) Shakespeare-like text, demonstrating the power of attention combined with sufficient model capacity.\n# Sample output from the trained GPTLanguageModel\nFlY BOLINGLO:\nThem thrumply towiter arts the\nmuscue rike begatt the sea it\nWhat satell in rowers that some than othis Marrity.\n\nLUCENTVO:\nBut userman these that, where can is not diesty rege;\nWhat and see to not. But's eyes. What?\nThis architecture—the decoder-only Transformer (using causal masking)—is fundamentally the same as that used in models like GPT-2 and GPT-3, just massively scaled up in terms of the number of parameters, layers, embedding sizes, and, crucially, the training data (vast amounts of internet text instead of just Shakespeare)."
  },
  {
    "objectID": "posts/transformer-attention/index.html#conclusion",
    "href": "posts/transformer-attention/index.html#conclusion",
    "title": "Understanding Self-Attention",
    "section": "Conclusion",
    "text": "Conclusion\nThe attention mechanism, specifically scaled dot-product self-attention, is the innovation that unlocked the power of Transformers. It allows tokens in a sequence to dynamically query each other, compute relevance scores (affinities) based on learned Query-Key interactions, and aggregate information from relevant tokens’ Value vectors in a weighted manner. Combined with Multi-Head Attention, Residual Connections, Layer Normalization, and position-wise FeedForward networks, it forms the Transformer block – the fundamental building block of the models revolutionizing AI today.\nBy building it up step-by-step, as Karpathy demonstrates, we see that while powerful, the core ideas are graspable and can be implemented with relatively concise code.\n\nThis post is based on Andrej Karpathy’s YouTube video “Let’s build GPT: from scratch, in code, spelled out.”. For the complete code and deeper insights, definitely check out the video and his nanogpt repository. Hopefully, this walkthrough helps clarify the magic behind Transformers and Attention!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Accelerating LLM Inference on Kubernetes: A Deep Dive into llm-d’s Architecture\n\n\n\nLLM\n\nLLM Inference\n\n\n\n\n\n\n\n\n\nNov 4, 2025\n\n\nJunichiro Iwasawa\n\n\n\n\n\n\n\n\n\n\n\n\nThe Interactive Turn: How DocETL and Dialog Engineering are Reshaping LLM Data Analysis\n\n\n\nLLM\n\nPodcast\n\n\n\n\n\n\n\n\n\nApr 25, 2025\n\n\nJunichiro Iwasawa\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Diffusion Models: From Core Concepts to Cutting-Edge Applications\n\n\n\nMachine Learning\n\nDiffusion models\n\n\n\n\n\n\n\n\n\nApr 17, 2025\n\n\nJunichiro Iwasawa\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Self-Attention\n\n\n\nMachine Learning\n\nTransformer\n\nPython\n\nLLM\n\n\n\n\n\n\n\n\n\nApr 11, 2025\n\n\nJunichiro Iwasawa\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/difusion-basics/index.html",
    "href": "posts/difusion-basics/index.html",
    "title": "Introduction to Diffusion Models: From Core Concepts to Cutting-Edge Applications",
    "section": "",
    "text": "In recent years, diffusion models have taken the generative modeling world by storm, particularly in image synthesis, often producing stunning results. This post aims to provide a comprehensive introduction, starting from the fundamental concepts and moving towards the advanced techniques that power today’s state-of-the-art models."
  },
  {
    "objectID": "posts/difusion-basics/index.html#what-are-diffusion-models",
    "href": "posts/difusion-basics/index.html#what-are-diffusion-models",
    "title": "Introduction to Diffusion Models: From Core Concepts to Cutting-Edge Applications",
    "section": "What are Diffusion Models?",
    "text": "What are Diffusion Models?\nDiffusion models are a class of generative models. While other approaches like Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Flow-based models have achieved significant success, they each come with their own challenges—GANs can suffer from training instability, VAEs rely on surrogate loss functions, and Flow models require specialized architectures for reversible transformations.\nInspired by non-equilibrium thermodynamics, diffusion models offer a different paradigm. They work through two main processes:\n\nForward Process (Diffusion Process): Gradually add small amounts of random noise (typically Gaussian) to the data over many steps, eventually transforming the data distribution into a simple, known distribution (like a standard Gaussian).\nReverse Process (Denoising Process): Learn to reverse the diffusion process. Starting from pure noise, incrementally remove the noise step-by-step to generate a sample that belongs to the original data distribution.\n\nThe core of the generative capability lies in the neural network trained to perform this “denoising” at each step. Diffusion models feature a fixed training procedure and, unlike VAEs or Flow models, typically operate with latent variables that have the same dimensionality as the original data.\n\nThe Forward Process: Turning Data into Noise\nStarting with an initial data point \\(\\mathbf{x}_0\\) drawn from the true data distribution \\(q(\\mathbf{x})\\), the forward process defines a Markov chain that adds Gaussian noise over \\(T\\) discrete time steps. The transition at each step \\(t\\) is defined as:\n\\[q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1}, \\beta_t\\mathbf{I})\\]\nHere, \\(\\\\{\\beta_t \\in (0, 1)\\\\}_{t=1}^T\\) is a variance schedule, a set of hyperparameters controlling the amount of noise added at each step. Typically, \\(\\beta_t\\) increases with \\(t\\), meaning more noise is added in later steps (common schedules include linear and cosine [Nichol & Dhariwal, 2021]). \\(\\mathbf{I}\\) is the identity matrix.\nThe joint distribution over all noisy samples given the initial data is:\n\\[q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0) = \\prod^T_{t=1} q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})\\]\nA key property of this process is that we can sample the noisy version \\(\\mathbf{x}_t\\) at any arbitrary timestep \\(t\\) directly from the original data \\(\\mathbf{x}_0\\) in a closed form. Defining \\(\\alpha_t = 1 - \\beta_t\\) and \\(\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i\\), the distribution of \\(\\mathbf{x}_t\\) given \\(\\mathbf{x}_0\\) is:\n\\[q(\\mathbf{x}_t \\vert \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t)\\mathbf{I})\\]\nThis can also be written using the reparameterization trick: \\(\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\boldsymbol{\\epsilon}\\), where \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\) is standard Gaussian noise. Essentially, \\(\\mathbf{x}_t\\) is a scaled version of the original data plus scaled noise. As \\(T \\to \\infty\\), \\(\\bar{\\alpha}_T \\approx 0\\), and \\(\\mathbf{x}_T\\) becomes almost pure Gaussian noise \\(\\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\), independent of the starting \\(\\mathbf{x}_0\\).\n\n\nThe Reverse Process: From Noise back to Data\nThe generative process reverses the forward diffusion. We start by sampling pure noise \\(\\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\) and then iteratively sample \\(\\mathbf{x}_{t-1}\\) given \\(\\mathbf{x}_t\\) for \\(t = T, T-1, \\dots, 1\\) to eventually obtain a sample \\(\\mathbf{x}_0\\).\nTo do this, we need the reverse transition probability \\(q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)\\). However, this distribution is intractable because it depends on the entire dataset. Therefore, we approximate it using a parameterized neural network, typically denoted as \\(p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)\\).\nThe entire reverse process is defined as:\n\\[p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T) \\prod^T_{t=1} p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)\\]\nwhere the starting noise distribution is \\(p(\\mathbf{x}_T) = \\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})\\). Each reverse transition step \\(p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)\\) is usually modeled as a Gaussian distribution:\n\\[p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t))\\]\nThe goal of the model is to learn the mean \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)\\) and the covariance \\(\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\) of this reverse transition. In practice, the covariance \\(\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)\\) is often not learned directly but is set to a fixed diagonal matrix \\(\\sigma_t^2 \\mathbf{I}\\). Common choices for \\(\\sigma_t^2\\) include \\(\\beta_t\\) (from the forward process) or \\(\\tilde{\\beta}_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t\\) (derived theoretically). While learning the variance was explored by [Nichol & Dhariwal, 2021] (e.g., as an interpolation between \\(\\beta_t\\) and \\(\\tilde{\\beta}_t\\)), it can sometimes lead to instability.\n\n\nThe Learning Objective: Predicting the Noise\nHow do we train the network to learn \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)\\)? While the full derivation involves maximizing the Variational Lower Bound (VLB) on the data log-likelihood, the DDPM paper [Ho et al., 2020] introduced a simpler, more intuitive objective that works remarkably well in practice.\nThe core idea is to reparameterize the model. Instead of directly predicting the mean \\(\\boldsymbol{\\mu}_\\theta\\) of the reverse step, the model is trained to predict the noise component \\(\\boldsymbol{\\epsilon}\\) that was added to the original data \\(\\mathbf{x}_0\\) to produce \\(\\mathbf{x}_t\\) during the forward process. Let’s denote this noise-predicting model as \\(\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\).\nUsing the relationship \\(\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\boldsymbol{\\epsilon}\\), we can express the (true) mean of the reverse step \\(\\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0)\\) (which we could compute if we knew \\(\\mathbf{x}_0\\)) in terms of this noise \\(\\boldsymbol{\\epsilon}\\). We train our model \\(\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\) to predict this true noise \\(\\boldsymbol{\\epsilon}\\), which in turn allows us to estimate the desired mean \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)\\).\nSpecifically, the learned mean \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)\\) is parameterized using the predicted noise \\(\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\) as follows:\n\\[\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\right)\\]\nThis equation shows that learning the noise prediction model \\(\\boldsymbol{\\epsilon}_\\theta\\) is sufficient to determine the mean of the reverse step.\nThe simplified training objective proposed in DDPM is then simply to minimize the Mean Squared Error (MSE) between the predicted noise \\(\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\) and the actual noise \\(\\boldsymbol{\\epsilon}\\) that was added:\n\\[L_\\text{simple} = \\mathbb{E}_{t \\sim \\mathcal{U}(1, T), \\mathbf{x}_0 \\sim q(\\mathbf{x}_0), \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})} \\left[\\|\\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}, t)\\|^2 \\right]\\]\nTraining Loop: 1. Sample a real data point \\(\\mathbf{x}_0 \\sim q(\\mathbf{x}_0)\\). 2. Sample a random timestep \\(t\\) uniformly from \\(\\\\{1, \\dots, T\\\\}\\). 3. Sample a standard Gaussian noise \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\). 4. Compute the noisy version \\(\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}\\). 5. Feed \\(\\mathbf{x}_t\\) and \\(t\\) into the model \\(\\boldsymbol{\\epsilon}_\\theta\\) to get the noise prediction \\(\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\). 6. Calculate the MSE loss between \\(\\boldsymbol{\\epsilon}\\) and \\(\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\). 7. Update the model parameters \\(\\theta\\) using gradient descent on this loss.\nConnection to Score Matching: Interestingly, this noise prediction task is closely related to score matching. The score function of a distribution \\(q(\\mathbf{x})\\) is defined as the gradient of its log-probability density, \\(\\nabla_{\\mathbf{x}} \\log q(\\mathbf{x})\\). The predicted noise \\(\\boldsymbol{\\epsilon}_\\theta\\) is approximately proportional to the score of the noisy data distribution at time \\(t\\): \\(\\mathbf{s}_\\theta(\\mathbf{x}_t, t) \\approx \\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t) \\approx - \\frac{\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)}{\\sqrt{1 - \\bar{\\alpha}_t}}\\). This links diffusion models to score-based generative models like NCSN [Song & Ermon, 2019]."
  },
  {
    "objectID": "posts/difusion-basics/index.html#evolution-and-applications-of-diffusion-models",
    "href": "posts/difusion-basics/index.html#evolution-and-applications-of-diffusion-models",
    "title": "Introduction to Diffusion Models: From Core Concepts to Cutting-Edge Applications",
    "section": "Evolution and Applications of Diffusion Models",
    "text": "Evolution and Applications of Diffusion Models\nFollowing the success of DDPM, research has focused on improving performance, expanding capabilities, and addressing limitations.\n\nConditional Generation\nGenerating samples based on specific information like class labels, text descriptions, or other images.\n\nClassifier Guidance: Proposed by [Dhariwal & Nichol, 2021]. This method uses a separately trained classifier \\(f_\\phi(y \\vert \\mathbf{x}_t)\\) that predicts the class label \\(y\\) given a noisy input \\(\\mathbf{x}_t\\). During generation, the gradient of the classifier’s log-likelihood \\(\\nabla_{\\mathbf{x}_t} \\log f_\\phi(y \\vert \\mathbf{x}_t)\\) is used to “guide” the noise prediction towards the desired class. The modified noise prediction is: \\[\\bar{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{x}_t, t) = \\boldsymbol{\\epsilon}_\\theta(x_t, t) - w \\sqrt{1 - \\bar{\\alpha}_t} \\nabla_{\\mathbf{x}_t} \\log f_\\phi(y \\vert \\mathbf{x}_t)\\] where \\(w\\) is a guidance scale factor. This was used in models like ADM (Ablated Diffusion Model) and ADM-G (ADM with Guidance).\nClassifier-Free Guidance (CFG): Proposed by [Ho & Salimans, 2021]. This popular technique avoids the need for a separate classifier. The diffusion model \\(\\boldsymbol{\\epsilon}_\\theta\\) itself is trained to handle both conditional inputs (\\(\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t, y)\\)) and unconditional inputs (\\(\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t, \\varnothing)\\), where \\(y=\\varnothing\\) represents the null condition). This is often achieved by randomly dropping the condition \\(y\\) during training. At inference time, guidance is achieved by extrapolating from the conditional and unconditional predictions: \\[\\bar{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{x}_t, t, y) = \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t, \\varnothing) + w (\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t, y) - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t, \\varnothing))\\] This can also be written as \\((w+1) \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t, y) - w \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t, \\varnothing)\\). CFG is widely used in modern high-performance models like Imagen, Stable Diffusion, and GLIDE. [Nichol et al., 2022] (GLIDE) found CFG to yield better results than guidance based on CLIP embeddings.\n\n\n\nSpeeding Up Sampling\nThe primary drawback of early diffusion models was the slow sampling speed due to the large number of steps (\\(T\\)). Significant progress has been made here.\n\nDDIM (Denoising Diffusion Implicit Models): Proposed by [Song et al., 2020]. While sharing the same forward process as DDPM, DDIM defines a non-Markovian generative process that allows for much larger step sizes. This makes the sampling deterministic (controlled by a parameter \\(\\eta\\); \\(\\eta=0\\) for DDIM, \\(\\eta=1\\) approximates DDPM) and significantly faster (e.g., reducing 1000 steps to 20-50) while maintaining high sample quality. Its deterministic nature also ensures “consistency” (same noise yields same image) and enables latent space interpolation.\nProgressive Distillation: Proposed by [Salimans & Ho, 2022]. This technique distills a trained deterministic sampler (like DDIM) into a new “student” model that takes half the number of steps. The student learns to perform two steps of the “teacher” model in a single step. This process can be repeated, exponentially reducing sampling time.\nConsistency Models: Proposed by [Song et al., 2023]. These models learn a function \\(f(\\mathbf{x}_t, t) \\approx \\mathbf{x}_0\\) that directly maps any point \\(\\mathbf{x}_t\\) on a diffusion trajectory back to its origin (or near-origin \\(\\mathbf{x}_\\epsilon\\)). They possess a “self-consistency” property. They can be trained either by distilling a pre-trained diffusion model (Consistency Distillation, CD) or from scratch (Consistency Training, CT). They hold the potential for high-quality generation in very few steps, even just one.\nLatent Diffusion Models (LDM): Proposed by [Rombach et al., 2022]. Instead of operating directly in the high-dimensional pixel space, LDMs first use a powerful autoencoder (Encoder \\(\\mathcal{E}\\), Decoder \\(\\mathcal{D}\\)) to compress the image \\(\\mathbf{x}\\) into a lower-dimensional latent representation \\(\\mathbf{z} = \\mathcal{E}(\\mathbf{x})\\). The diffusion process (often using a U-Net) is then applied entirely within this latent space. To generate an image, noise is denoised in the latent space to produce \\(\\mathbf{z}\\), which is then mapped back to the pixel space using the decoder \\(\\mathcal{D}(\\mathbf{z})\\). This drastically reduces computational cost and memory requirements, forming the basis for models like Stable Diffusion. Regularization techniques like KL penalty or Vector Quantization (VQ) are used in the autoencoder training. Conditioning is often implemented using cross-attention mechanisms within the latent U-Net.\n\n\n\nAchieving Higher Resolution and Quality\n\nCascaded Models: Employed by [Ho et al., 2021] and others. This involves a pipeline approach: first generate a low-resolution image, then use one or more super-resolution diffusion models conditioned on the low-resolution output to generate progressively higher-resolution images. “Noise Conditioning Augmentation” (adding noise to the low-resolution conditioning input) was found crucial for improving quality by mitigating error accumulation.\nunCLIP / DALL-E 2: Proposed by [Ramesh et al., 2022]. These models leverage the powerful CLIP model for high-quality text-to-image generation. They typically involve a two-stage process: (1) A “prior” model generates a CLIP image embedding \\(\\mathbf{c}^i\\) conditioned on the input text \\(y\\) (\\(P(\\mathbf{c}^i \\vert y)\\)). (2) A “decoder” diffusion model generates the final image \\(\\mathbf{x}\\) conditioned on the image embedding \\(\\mathbf{c}^i\\) (and optionally the text \\(y\\)) (\\(P(\\mathbf{x} \\vert \\mathbf{c}^i, [y])\\)).\nImagen: Proposed by [Saharia et al., 2022]. Instead of CLIP, Imagen uses large, pre-trained frozen language models (like T5-XXL) as text encoders, finding that the scale of the text encoder was more critical than the scale of the diffusion U-Net. It introduced “Dynamic Thresholding” to improve image fidelity at high CFG scales by adaptively clipping predicted pixel values. It also proposed an “Efficient U-Net” architecture with modifications like shifting parameters to lower-resolution blocks and optimizing the order of up/downsampling operations.\nArchitectural Evolution (U-Net, DiT, ControlNet):\n\nU-Net: The classic architecture with downsampling/upsampling paths and skip connections remains a standard backbone for many diffusion models, especially in image domains.\nDiT (Diffusion Transformer): Proposed by [Peebles & Xie, 2023]. Adapts the Transformer architecture for diffusion, operating on latent patches (similar to LDM). It involves patchifying the latent representation, processing the sequence of patches through Transformer blocks, and incorporating conditioning (like timestep \\(t\\) and class \\(c\\)) via adaptive layer normalization (adaLN-Zero). DiT benefits from the known scalability of Transformers.\nControlNet: Proposed by [Zhang et al., 2023]. A technique for adding fine-grained spatial control (e.g., based on edge maps, human poses, depth maps) to large, pre-trained text-to-image diffusion models without expensive retraining. It works by creating a trainable copy of the model’s weights and connecting it to the original frozen weights via special “zero convolution” layers. These zero-initialized layers allow stable training of the control mechanism while preserving the original model’s capabilities. The output combines the original block’s output with the controlled copy’s output: \\(\\mathbf{y}_c = \\mathcal{F}_\\theta(\\mathbf{x}) + \\mathcal{Z}_{\\theta_{z2}}(\\mathcal{F}_{\\theta_c}(\\mathbf{x} + \\mathcal{Z}_{\\theta_{z1}}(\\mathbf{c})))\\)."
  },
  {
    "objectID": "posts/difusion-basics/index.html#summary",
    "href": "posts/difusion-basics/index.html#summary",
    "title": "Introduction to Diffusion Models: From Core Concepts to Cutting-Edge Applications",
    "section": "Summary",
    "text": "Summary\nDiffusion models represent a powerful and flexible class of generative models based on systematically destroying data structure with noise and then learning to reverse the process.\n\nAdvantages: They achieve state-of-the-art results in generating high-quality, diverse samples, particularly for images. They benefit from theoretical tractability and relatively stable training compared to alternatives like GANs.\nDisadvantages: Historically, their main drawback was slow sampling speed, requiring many sequential denoising steps. However, significant advancements (DDIM, LDM, distillation, consistency models) have drastically improved sampling efficiency, making them much more practical.\n\nFueled by innovations like Classifier-Free Guidance, Latent Diffusion, Transformer-based architectures, and control mechanisms like ControlNet, diffusion models are at the forefront of generative AI, enabling cutting-edge applications in text-to-image synthesis, image editing, video generation, and beyond."
  },
  {
    "objectID": "posts/difusion-basics/index.html#references",
    "href": "posts/difusion-basics/index.html#references",
    "title": "Introduction to Diffusion Models: From Core Concepts to Cutting-Edge Applications",
    "section": "References",
    "text": "References\n\nWeng, Lilian. (Jul 2021). What are diffusion models? Lil’Log. https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\nHo, Jonathan, Ajay Jain, and Pieter Abbeel. “Denoising diffusion probabilistic models.” NeurIPS 2020. (DDPM)\nSong, Jiaming, Chenlin Meng, and Stefano Ermon. “Denoising diffusion implicit models.” ICLR 2021. (DDIM)\nRombach, Robin, et al. “High-resolution image synthesis with latent diffusion models.” CVPR 2022. (Latent Diffusion / Foundation for Stable Diffusion)\nNichol, Alex, and Prafulla Dhariwal. “Improved denoising diffusion probabilistic models.” ICML 2021.\nDhariwal, Prafulla, and Alex Nichol. “Diffusion models beat gans on image synthesis.” NeurIPS 2021.\nHo, Jonathan, and Tim Salimans. “Classifier-free diffusion guidance.” NeurIPS 2021 Workshop.\nSalimans, Tim, and Jonathan Ho. “Progressive distillation for fast sampling of diffusion models.” ICLR 2022.\nSong, Yang, et al. “Consistency models.” ICML 2023.\nHo, Jonathan, et al. “Cascaded diffusion models for high fidelity image generation.” JMLR 2022.\nRamesh, Aditya, et al. “Hierarchical text-conditional image generation with clip latents.” arXiv 2022. (unCLIP / DALL-E 2)\nSaharia, Chitwan, et al. “Photorealistic text-to-image diffusion models with deep language understanding.” NeurIPS 2022. (Imagen)\nPeebles, William, and Saining Xie. “Scalable diffusion models with transformers.” ICCV 2023. (DiT)\nZhang, Lvmin, and Maneesh Agrawala. “Adding conditional control to text-to-image diffusion models.” ICCV 2023. (ControlNet)"
  },
  {
    "objectID": "posts/docetl/index.html",
    "href": "posts/docetl/index.html",
    "title": "The Interactive Turn: How DocETL and Dialog Engineering are Reshaping LLM Data Analysis",
    "section": "",
    "text": "Large Language Models (LLMs) hold immense promise for extracting insights from the vast oceans of unstructured data we navigate daily. Yet, anyone who has tried to apply them to complex, real-world tasks knows the reality is often messy. Optimizing LLM pipelines for accuracy and efficiency, especially at scale, can quickly become a frustrating exercise in manual tuning.\nEnter DocETL, a framework developed by UC Berkeley researcher Shreya Shankar and colleagues (paper available here), which has been garnering attention. DocETL aims to tame this complexity. Insights from Shankar’s recent interview on the TWIML Podcast shed light not only on DocETL’s innovative approach but also on a more fundamental truth about how we need to work with LLMs productively.\n\nThe Harsh Reality of LLM Data Processing: Beyond Slick Demos\nAs detailed in the interview and the DocETL announcement blog post, simply throwing a complex task at an LLM often yields disappointing results. Consider analyzing decades of presidential debate transcripts for evolving themes. The sheer scale can overwhelm context windows. The complexity involves more than extraction – it requires identifying themes, tracking changes over time, and synthesizing viewpoints across documents. And the ever-present challenge of accuracy means dealing with hallucinations and missed information.\nShankar’s work on another project analyzing police misconduct records in California highlights the high stakes. Sifting through potentially thousands of pages of unstructured text for patterns requires precision; mistakes are not an option, yet traditional manual annotation is incredibly time-consuming.\nMany developers attempt to solve this by manually chunking data, crafting intricate prompts, and orchestrating multiple LLM calls. But as Shankar points out, this often leads to brittle pipelines that are difficult to modify and may only yield mediocre results after days of painstaking effort.\n\n\nDocETL: A Declarative Framework with LLM Agent-Powered Optimization\nDocETL offers a different path: a declarative framework for building and optimizing LLM-powered data processing pipelines. Users define their desired operations – like Map (apply to each document), Reduce (aggregate results), Split (chunk documents), Gather (add context to chunks), or Resolve (normalize similar entities) – along with prompts describing the task for each step, using YAML or Python.\nThe core innovation lies beyond simple execution. DocETL employs LLM agents to automatically rewrite and optimize the user’s initial pipeline for better accuracy. This involves two key steps:\n\nPipeline Rewriting: Based on predefined rules (covering data decomposition, adding intermediate steps, and LLM-specific improvements), DocETL’s agents propose alternative pipeline structures. For instance, a complex Map operation might be automatically broken down into a sequence: split the document, gather relevant context for each chunk, apply a simpler map to each chunk, and then aggregate the results.\nQuality Assessment & Selection: The agents generate task-specific validation criteria (e.g., “Were all instances of misconduct extracted?”, “Can each extracted instance be traced back to the source document?”). They then use an “LLM-as-a-judge” approach to evaluate the outputs of different candidate pipelines on sample data, ultimately selecting the plan predicted to yield the highest accuracy.\n\nThis approach frees analysts from wrestling with low-level implementation details like optimal chunk sizes or error recovery logic, allowing them to focus on their analytical goals.\n\n\nThe Inescapable Need for “Interaction”\nPerhaps the most compelling insight from Shankar’s interview transcends DocETL’s technical architecture. She repeatedly emphasizes the crucial role of interaction in working effectively with LLMs. The idea of writing a perfect, one-shot prompt is often a myth.\n\n“Users have no idea how to write the prompt until they see the initial output,” Shankar explains. “They have to see what the LLM came up with the first time… and then say ‘Oh, like actually…’ They changed the task; they redefine [e.g.] misconduct.”\n\n\n“The more users iterate [based on seeing intermediate outputs], the more complex the prompts get… That’s very fascinating to me because I think there’s a lot of work in automated prompt engineering… that really puts the human out [of the loop].”\n\nIt’s through seeing the LLM’s output – its successes, failures, and ambiguities – that users truly understand the nuances of their task and refine their instructions. This human-driven iterative refinement process, Shankar suggests, is fundamental to successfully leveraging LLMs for complex problems.\n\n\nEchoes of Jeremy Howard’s “Dialog Engineering”\nShankar’s findings resonate strongly with the concept of “Dialog Engineering,” championed by Jeremy Howard, co-founder of fast.ai and now leading Answer.AI. Jeremy argues that the common approach of throwing large, monolithic prompts at an AI and hoping for hundreds of lines of perfect code is fundamentally flawed for real-world development.\nDialog Engineering proposes the opposite: a tight interactive loop between human and LLM, where code or other artifacts are co-constructed in small, manageable increments. Validation happens at each step.\nAnswer.AI’s tool, solveit (currently in private beta), aims to embody this philosophy. It provides an interface blending chat and a REPL (Read-Eval-Print Loop), allowing users to give instructions in natural language, receive small code suggestions, and immediately execute and verify them within the same environment. The LLM always sees the current context (conversation, files), and users can easily step back or re-run parts of the process if things go awry or requirements change. Simple tests can even be embedded directly in the conversation flow to continuously check for regressions.\nThe development style solveit enables—stating a small goal, getting a suggestion, immediately testing it, then accepting or refining—directly mirrors the iterative process Shankar observed as essential in her DocETL research. Both DocETL’s findings and solveit’s approach point to the same conclusion: we need to treat LLMs less like black-box instruction takers and more like collaborative partners engaged in a dialog. Shankar’s research provides compelling empirical validation for the principles behind Dialog Engineering, extending their relevance beyond code generation to complex data analysis.\n\n\nThe Road Ahead: Challenges and Opportunities\nDocETL is still evolving, and Shankar acknowledges several areas for future work:\n\nInterfaces: Moving beyond YAML to more intuitive UIs that facilitate visualization and iteration on large documents and LLM outputs.\nAgent Reliability: Improving the consistency and fault tolerance of the LLM agents performing the optimization.\nOptimization Speed & Transparency: Making the optimization process faster and more debuggable for users.\nBenchmarking: Developing new benchmarks specifically designed to evaluate LLM performance on the complex, long-context data processing tasks DocETL targets.\n\n\n\nConclusion: Dialog is Key in the LLM Era\nDocETL represents a significant step forward in harnessing LLMs for complex unstructured data analysis. Its declarative framework and agent-based optimization offer a powerful alternative to manual pipeline construction. However, the research journey itself highlights a critical lesson: the technology alone isn’t enough.\nThe true potential of LLMs in these domains will be unlocked not just by better models or automation, but by better interfaces and workflows that embrace human-LLM interaction. We need tools that facilitate the back-and-forth, the iterative refinement, the dialog that allows us to clarify our intent and leverage the LLM’s capabilities effectively. The direction indicated by DocETL’s research and tools like Jeremy Howard’s solveit suggests that this interactive paradigm is central to the future of AI-assisted analysis and development."
  },
  {
    "objectID": "posts/llm-d-anatomy/index.html",
    "href": "posts/llm-d-anatomy/index.html",
    "title": "Accelerating LLM Inference on Kubernetes: A Deep Dive into llm-d’s Architecture",
    "section": "",
    "text": "The explosive growth of Large Language Models (LLMs) has been incredible, but serving these massive models in a production environment is a formidable challenge. The core difficulty lies in a constant battle: balancing the demand for low latency with the need for high throughput.\nllm-d has emerged as a powerful solution to this problem. It’s a Kubernetes-native distributed inference serving stack that provides “Well-lit Paths”—battle-tested recipes—for scaling large generative AI models.\nThis article explores the architecture of llm-d, breaking down how it integrates powerful open-source components like Kubernetes, vLLM, and the Envoy proxy to maximize LLM inference efficiency."
  },
  {
    "objectID": "posts/llm-d-anatomy/index.html#the-core-architecture-of-llm-d",
    "href": "posts/llm-d-anatomy/index.html#the-core-architecture-of-llm-d",
    "title": "Accelerating LLM Inference on Kubernetes: A Deep Dive into llm-d’s Architecture",
    "section": "The Core Architecture of llm-d",
    "text": "The Core Architecture of llm-d\nAt its heart, the llm-d architecture is composed of three distinct component layers working in concert:\n\nInference Scheduler (The Smart Traffic Cop)\n\nBuilt on the Kubernetes Inference Gateway (IGW) and Envoy proxy, this layer acts as an intelligent traffic director.\nInstead of simple round-robin load balancing, it makes “smart” routing decisions based on the real-time state of the vLLM servers, including load and KV cache contents.\n\nvLLM Model Servers (The Inference Engine)\n\nThese are the workhorses that actually run the LLM models and generate text.\nThey can be configured as single-host or multi-host deployments and are the execution endpoint for llm-d’s advanced optimizations.\n\nKubernetes (The Orchestrator)\n\nKubernetes serves as the foundation, managing the infrastructure and control plane.\nIt handles the deployment, scaling, and resource management for all of llm-d’s components.\n\n\nThe key insight of llm-d is its intelligent routing layer. It’s a scheduler that understands LLM-specific concerns—like “which server has this prompt’s prefix already cached?”—and integrates this logic seamlessly with the robust orchestration of Kubernetes."
  },
  {
    "objectID": "posts/llm-d-anatomy/index.html#the-three-well-lit-paths-to-optimization",
    "href": "posts/llm-d-anatomy/index.html#the-three-well-lit-paths-to-optimization",
    "title": "Accelerating LLM Inference on Kubernetes: A Deep Dive into llm-d’s Architecture",
    "section": "The three “Well-lit Paths” to Optimization",
    "text": "The three “Well-lit Paths” to Optimization\nllm-d provides three primary optimization patterns (or “paths”) tailored to different use cases.\n\n1. Intelligent Inference Scheduling\nThis is the most fundamental and broadly applicable optimization path.\n\nThe Problem: Traditional load balancers are blind. They don’t know which vLLM server is busy or, more importantly, which server already has the KV cache for a given prompt. This leads to cache misses and inefficient resource use.\nThe Solution: The llm-d Inference Gateway “scores” each vLLM instance before routing a request.\n\nLoad-Awareness: It uses a queue-scorer to check the depth of the request queue and avoid overloading busy servers.\nKV-Cache-Awareness: It uses a precise-prefix-cache-scorer to identify the server most likely to have the request’s prefix already in its cache, dramatically increasing cache hit rates.\n\n\nThis scoring is highly customizable through weighted plugins in the configuration:\n# From gaie-kv-events/values.yaml\nschedulingProfiles:\n  - name: default\n    plugins:\n      # Prioritize prefix cache hits\n      - pluginRef: precise-prefix-cache-scorer\n        weight: 3.0\n      # Factor in overall KV cache memory usage\n      - pluginRef: kv-cache-utilization-scorer\n        weight: 2.0\n      # Factor in the request queue length\n      - pluginRef: queue-scorer\n        weight: 2.0\n      - pluginRef: max-score-picker\nTo make this work, vLLM instances publish their cache status to the scheduler via ZMQ (a messaging protocol), ensuring the scheduler always has a real-time view for making optimal routing decisions.\n\n\n2. Prefill/Decode (P/D) Disaggregation\nThis technique is especially powerful for large models (like Llama-70B+) and use cases involving long prompts.\n\nThe Problem: LLM inference is a two-phase process:\n\nPrefill: Processing the entire input prompt (compute-intensive).\nDecode: Generating output tokens one by one (memory-intensive). These two phases have vastly different resource profiles. Running them on the same GPU is inefficient and leads to unstable latency.\n\nThe Solution: llm-d splits these roles into separate Kubernetes Deployments.\n\nPrefill Workers: A group of pods, each with fewer GPUs (e.g., 4 pods, 1 GPU each), to handle many incoming prompts in parallel.\nDecode Workers: A single pod (or few pods) with many GPUs using Tensor Parallelism (e.g., 1 pod, 4 GPUs) to accommodate the large model.\n\n\nWhen a request arrives, a Prefill worker computes the KV cache and transfers it over a high-speed interconnect (like RDMA or InfiniBand, via NIXL) to a Decode worker, which then takes over token generation.\n# A simplified example from ms-pd/values.yaml\n\n# Decode Worker Configuration\ndecode:\n  parallelism:\n    tensor: 4  # 4-way Tensor Parallelism\n  replicas: 1    # Only 1 replica\n  resources:\n    nvidia.com/gpu: \"4\" # Requests 4 GPUs\n    rdma/ib: 1\n\n# Prefill Worker Configuration\nprefill:\n  # No parallelism (TP=1)\n  replicas: 4    # 4 replicas to run in parallel\n  resources:\n    nvidia.com/gpu: \"1\" # Requests 1 GPU\n    rdma/ib: 1\nCaveat: This approach adds overhead from the KV cache transfer. It’s counterproductive for short prompts. llm-d accounts for this with “Selective PD,” using a threshold parameter to bypass P/D for prompts below a certain token count.\n\n\n3. Wide Expert-Parallelism (for MoE Models)\nThis is the most advanced path, designed for Mixture-of-Experts (MoE) models like DeepSeek-R1.\n\nThe Problem: MoE models are enormous, but only a fraction of their “experts” are activated for any given token.\nThe Solution: This path extends the P/D concept by using Data Parallelism (DP) to spread the model’s experts across a large cluster of GPUs (e.g., 24+), using high-speed networking for communication between the experts."
  },
  {
    "objectID": "posts/llm-d-anatomy/index.html#leveraging-kubernetes-patterns",
    "href": "posts/llm-d-anatomy/index.html#leveraging-kubernetes-patterns",
    "title": "Accelerating LLM Inference on Kubernetes: A Deep Dive into llm-d’s Architecture",
    "section": "Leveraging Kubernetes Patterns",
    "text": "Leveraging Kubernetes Patterns\nllm-d is “Kubernetes-native” because it masterfully uses Kubernetes’ standard resources to build its complex architecture.\n\nDeployment: Used for all long-running services (vLLM, Envoy Gateway, EPP scheduler) to ensure they are self-healing and restart automatically.\nService: Provides stable network access to pods.\n\nLoadBalancer Service: Assigned to the Envoy Gateway, creating a single, external IP address for clients to send inference requests.\nClusterIP Service: Used for all internal components (vLLM pods, EPP), ensuring they can only be reached from within the cluster.\n\nHTTPRoute: A resource from the Kubernetes Gateway API that acts as the “glue,” connecting the external-facing Gateway to the internal InferencePool (which holds the scheduling logic)."
  },
  {
    "objectID": "posts/llm-d-anatomy/index.html#monitoring-and-troubleshooting",
    "href": "posts/llm-d-anatomy/index.html#monitoring-and-troubleshooting",
    "title": "Accelerating LLM Inference on Kubernetes: A Deep Dive into llm-d’s Architecture",
    "section": "Monitoring and Troubleshooting",
    "text": "Monitoring and Troubleshooting\nllm-d is built for production, which means deep observability via Prometheus and Grafana is standard.\nKey Metrics to Watch:\n\nvllm:time_to_first_token_seconds_bucket: Your key latency metric (TTFT).\nvllm:prefix_cache_hits_total / vllm:prefix_cache_queries_total: The KV cache hit rate. A low rate means high re-computation.\nvllm:num_requests_waiting: The depth of the vLLM request queue.\nvllm:kv_cache_usage_perc: The GPU memory pressure from the KV cache.\n\nCommon Troubleshooting Scenario: High Latency\n\nCheck vllm:num_requests_waiting: If this metric is high and climbing, your vLLM replicas are overwhelmed. You need to scale up your deployment.\nCheck vllm:kv_cache_usage_perc: If this is hovering near 100%, your cache is “thrashing”—constantly evicting old entries to make room for new ones. This forces re-computation and kills performance. You may need to optimize your scheduler weights or increase GPU memory."
  },
  {
    "objectID": "posts/llm-d-anatomy/index.html#conclusion",
    "href": "posts/llm-d-anatomy/index.html#conclusion",
    "title": "Accelerating LLM Inference on Kubernetes: A Deep Dive into llm-d’s Architecture",
    "section": "Conclusion",
    "text": "Conclusion\nllm-d is not a single tool, but a comprehensive stack of “Well-lit Paths” for high-performance LLM inference on Kubernetes.\nIt packages advanced optimization techniques—like intelligent, cache-aware scheduling and P/D disaggregation—into a robust framework that leverages Kubernetes-native APIs, Helm for deployment, and Prometheus for observability. For any team serious about serving large-scale AI in production, llm-d represents a powerful and well-architected solution."
  }
]